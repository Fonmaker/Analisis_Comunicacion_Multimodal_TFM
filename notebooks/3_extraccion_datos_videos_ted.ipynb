{"cells":[{"cell_type":"markdown","source":["# Extracción y procesamiento de datos multimodales en vídeos TED\n","\n","Este notebook implementa el pipeline completo descrito en el apartado **3.2 del TFM**, correspondiente al **procesamiento de vídeos y construcción del dataset multimodal**.\n","\n","Incluye tareas de descarga, transcripción automática, segmentación dinámica, extracción de características acústicas y visuales, y organización de los resultados por vídeo y por lote.\n","\n","Los datos procesados se almacenan en formato estructurado (JSON), y posteriormente integrados en estructuras tabulares para su análisis.\n"],"metadata":{"id":"VAvbu5QUrVeQ"}},{"cell_type":"markdown","source":["⚠️ Requisitos importantes antes de ejecutar este notebook\n","Este notebook está diseñado para ejecutarse en Google Colab.\n","Requiere el uso de una GPU T4 y una correcta configuración de rutas para funcionar correctamente.\n","\n","1. Tipo de entorno\n","Ve a Entorno de ejecución > Cambiar tipo de entorno y selecciona GPU T4\n","Comprueba que se ha asignado una GPU T4 ejecutando:\n","2. Instalación de librerías\n","Tras la instalación de las librerías necesarias, es obligatorio reiniciar el entorno antes de continuar con la ejecución.\n","\n","Ve a Entorno de ejecución > Reiniciar entorno de ejecución\n","\n","3. Configuración de rutas\n","Antes de ejecutar las celdas principales, configura correctamente las siguientes rutas:\n","\n","folder_path: Carpeta de trabajo principal.\n","\n","json_path: Carpeta donde descargaremos los json, a meddida que se vayan analizando los videos.\n","\n","model_path: Carpeta donde está guardado el modelo de emociones audio\n"],"metadata":{"id":"M0DuBG2xw8Jd"}},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"id":"at_Z2Stvmown","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1757870274551,"user_tz":-120,"elapsed":21003,"user":{"displayName":"Alfonso Graña","userId":"05993286614178522597"}},"outputId":"9bf8693b-9782-4214-eb0d-04d2192cb736"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faster-whisper\n","  Downloading faster_whisper-1.2.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting ultralytics\n","  Downloading ultralytics-8.3.199-py3-none-any.whl.metadata (37 kB)\n","Collecting yt-dlp\n","  Downloading yt_dlp-2025.9.5-py3-none-any.whl.metadata (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n","  Downloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.34.4)\n","Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.0)\n","Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n","  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting av>=11 (from faster-whisper)\n","  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.67.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n","Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio) (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.19.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.4.0)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.9)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchaudio) (3.0.2)\n","Downloading faster_whisper-1.2.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics-8.3.199-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yt_dlp-2025.9.5-py3-none-any.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n","Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: yt-dlp, protobuf, humanfriendly, ctranslate2, av, sounddevice, coloredlogs, onnxruntime, ultralytics-thop, mediapipe, faster-whisper, ultralytics\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed av-15.1.0 coloredlogs-15.0.1 ctranslate2-4.6.0 faster-whisper-1.2.0 humanfriendly-10.0 mediapipe-0.10.14 onnxruntime-1.22.1 protobuf-4.25.8 sounddevice-0.5.2 ultralytics-8.3.199 ultralytics-thop-2.0.17 yt-dlp-2025.9.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"ec01847c3f744c6f82e9cf791aca15db"}},"metadata":{}}],"source":["!pip install faster-whisper librosa torchaudio ultralytics yt-dlp opencv-python pandas matplotlib mediapipe"]},{"cell_type":"markdown","metadata":{"id":"_HHMvIUmmowq"},"source":["# **IMPORTACIONES**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bjKxdh6kmowq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757870295115,"user_tz":-120,"elapsed":10541,"user":{"displayName":"Alfonso Graña","userId":"05993286614178522597"}},"outputId":"658af561-bcaa-43eb-8030-a1b1290d4b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["import os\n","import warnings\n","\n","# ---- Ignorar warnings de Python ----\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","import torch\n","import torchaudio\n","import librosa\n","import joblib\n","import pandas as pd\n","from ultralytics import YOLO\n","import mediapipe as mp\n","\n","\n","import time\n","import json\n","import numpy as np\n","import subprocess\n","import tempfile\n","from faster_whisper import WhisperModel\n","import yt_dlp\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import math\n","import random\n","import cv2\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## Configuración del entorno y rutas de trabajo\n","\n","Se definen las rutas de entrada y salida donde se almacenarán los vídeos descargados, las transcripciones, las características extraídas y los resultados finales.\n","\n","También se configuran los parámetros globales del procesamiento de audio (frecuencia de muestreo, tamaño de ventana, etc.).\n"],"metadata":{"id":"V64cleYTxHnz"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"ulX4mqYlnMJm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757870304131,"user_tz":-120,"elapsed":1449,"user":{"displayName":"Alfonso Graña","userId":"05993286614178522597"}},"outputId":"5479f03f-9232-4267-e859-33fecde039d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#carpeta de trabajo principal\n","folder_path = \"/content/drive/MyDrive/Analisis_Multimodal_Comunicacion_TFM/data/folder_path\"\n","#carpeta para guardar los archivos conforme se vallan analizando videos\n","json_path = \"/content/drive/MyDrive/Analisis_Multimodal_Comunicacion_TFM/data/json_path\"\n","#carpeta donde está el modelo de emodiones del audio\n","model_path = \"/content/drive/MyDrive/Analisis_Multimodal_Comunicacion_TFM/models\"\n","\n","\n","\n","os.makedirs(folder_path, exist_ok=True)\n","os.makedirs(json_path, exist_ok=True)\n","os.makedirs(model_path, exist_ok=True)"]},{"cell_type":"markdown","source":["## Configuración del dispositivo y carga del modelo Whisper\n","\n","Se detecta automáticamente si hay una GPU disponible para acelerar el procesamiento. Luego se inicializa el modelo Whisper (`small`) para realizar la transcripción automática de los vídeos, con soporte para múltiples idiomas.\n","\n","Este modelo se usará más adelante para obtener la transcripción cronometrada de cada vídeo, paso fundamental para la segmentación dinámica y análisis textual.\n"],"metadata":{"id":"aA1Wa_mxxPC8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgQICUxbmowr","colab":{"base_uri":"https://localhost:8080/","height":327,"referenced_widgets":["2b824f5050a742ed95247bbcfd5f0ad0","7c8856038aea43a7baa1efb359ee3ac0","e33d28422d9444bab1806c480a804c76","ea052b8f1fc246f2bd4b9ed53dc4c7b4","a5f90fe2368946789b039562b7271ced","2b6f5be8b7714e3ebdabd4bd92bb00ec","c1fb0fe7573d44f6bd39afadfca053e7","f75bd02f6e704d2192a02b225e4826a5","ab90f0b229574fcda8457ee7edc41ffe","c388c285f434486a9b006e60265ae915","15184d71dead470eb609f059ab393450","d88320745e3144aeb3bb37ef615f32d8","9460d1b1f84d4bb49fb4d5e9f0ee5d1b","1db6cef6b5dd49b6bf31dbcd5abe4e87","5078a2a45b554b6c8b20374b948fd514","5774bbf3b3a34ac185c974f0bded1644","36e1469c10294847a7b6e525c3b6160b","13cbe7d2b6364adc81ce3c1f35183038","11ea737390184d6bafde270b2ae774e3","3c308a69a798424794fdc6089233dd48","f89a3cccbc69436497fc0c3d4e977c19","3ee42cb92b7b4c85aa9ca5a92f52727b","d518605723d44bf19b7a8c0e87ee25f4","0ffddf6122ce4d7886f15a2ee628b371","2dab8b99c7e2484c835a1c3cdd509317","6c00a587f52141cfbcf6c7f0b4e2b289","d5d2643e7e814e428d0a168842cfdc97","0b82ad0fc4bd43c19dab6fa65f52d43c","d90476cf0de3475ca52c98e2bc8dd604","b79aa47d0bc8474ab288af4758d2080c","6c524f727588442a97e99f4de1b89cdb","583f267b419c4c6b986fb47b55300bf4","e217b7c8c5254f258c8e2259e8f68782","832dbe7906194125ac982be3254548e3","d9d61acaa48b469ab7c6668ac4d0c435","5b532466f07d4d55bd305ba717d2e09b","b425279d57d54ac79089cc6ae41f90ac","28e767265f284cd4bfc391664ea75aa6","6c4ad304280d4698a841c1ad533d7cc4","f940f82a28ef423ca7a4928848d60f69","637666b8c0fb45249199f4a72e9d8050","15d3547b70804bd69a8d71129e4a5c33","29060677ad8e4c4db5c67df81f84cbd0","c9f4b545053d4b1c8f1153051684351b"]},"executionInfo":{"status":"ok","timestamp":1757668179142,"user_tz":-120,"elapsed":4681,"user":{"displayName":"Alfonso Graña","userId":"05993286614178522597"}},"outputId":"78328fdc-e1bf-4874-cf2c-c98140ab6b49"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","Configuración de Dispositivo:\n","Tipo: CUDA\n","GPU: NVIDIA A100-SXM4-80GB\n","Capacidad: (8, 0)\n","Memoria Total: 79.32 GB\n","==================================================\n","\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b824f5050a742ed95247bbcfd5f0ad0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocabulary.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88320745e3144aeb3bb37ef615f32d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d518605723d44bf19b7a8c0e87ee25f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.bin:   0%|          | 0.00/484M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832dbe7906194125ac982be3254548e3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Modelo Whisper configurado en cuda con compute_type=float32\n"]}],"source":["# =====================\n","# Config\n","# =====================\n","SAMPLE_RATE = 16000\n","FRAME_LENGTH = 2048\n","HOP_LENGTH = 512\n","EMPHASIS_LEVELS = 10\n","\n","# =====================\n","# Definir device\n","# =====================\n","\n","# Configuración inicial\n","device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(device_type)\n","\n","# Verificación detallada del dispositivo\n","print(f\"\\n{'='*50}\")\n","print(f\"Configuración de Dispositivo:\")\n","print(f\"Tipo: {device_type.upper()}\")\n","if device_type == \"cuda\":\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Capacidad: {torch.cuda.get_device_capability()}\")\n","    print(f\"Memoria Total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n","print(f\"{'='*50}\\n\")\n","\n","# Usar float32 por defecto para mayor estabilidad\n","torch.set_default_dtype(torch.float32)\n","compute_type = \"float32\"  # Para Whisper\n","\n","# =========================================\n","# Definir modelo para extracción del texto\n","# =========================================\n","\n","model_whisper = WhisperModel(\n","    \"small\",\n","    device=device_type,\n","    compute_type=compute_type,\n",")\n","print(f\"Modelo Whisper configurado en {device_type} con compute_type={compute_type}\")\n","\n","\n","def clean_up():\n","    if device_type == \"cuda\":\n","        torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","source":["  ## Modelo para la detección de emociones acústicas\n","\n","En este bloque se carga y aplica el modelo entrenado para predecir la emoción predominante en un segmento de audio, como se describe en el apartado **3.2.5 del TFM**.\n","\n","### Arquitectura del modelo\n","\n","Se define una red neuronal profunda (`DeepModel`) implementada con PyTorch. Su entrada es un vector de 143 características acústicas, y su salida corresponde a una de las 8 emociones posibles (según el dataset RAVDESS). La arquitectura incluye:\n","\n","- Capas totalmente conectadas (`Linear`) con activación ReLU.\n","- Regularización mediante `Dropout` en varias capas ocultas.\n","- Una capa de salida con tamaño igual al número de clases emocionales.\n","\n","El modelo se carga desde disco (`deep_model.pth`) y se pasa a modo evaluación (`eval()`), utilizando `float32` para mantener la estabilidad numérica.\n","\n","### Normalización y codificación\n","\n","Se cargan también dos objetos entrenados previamente:\n","- Un **`StandardScaler`** (`scaler.pkl`) para normalizar las características antes de la predicción.\n","- Un **`LabelEncoder`** (`label_encoder.pkl`) para decodificar las predicciones numéricas en etiquetas emocionales (por ejemplo, *alegría*, *tristeza*, etc.).\n","\n","### Funciones auxiliares\n","\n","Se definen dos funciones clave:\n","\n","- `pad_audio_smart()`: ajusta cualquier segmento de audio a una longitud fija (2.5 segundos) mediante recorte o padding (con ceros o parte del audio anterior), para asegurar una entrada consistente al modelo.\n","\n","- `extract_features_emotion()`: calcula y concatena las características acústicas que alimentan al modelo:\n","  - ZCR (Zero Crossing Rate)\n","  - MFCC (13 coeficientes)\n","  - RMS (energía)\n","  - Mel Spectrogram (resumen por bandas)\n","\n","- `predict_emotion()`: transforma el audio en características, las normaliza, las pasa por el modelo y devuelve la **emoción estimada como etiqueta** (texto).\n","\n","Esta función se aplicará a cada segmento de audio extraído del corpus TED, generando el valor `emocion_audio` que se"],"metadata":{"id":"76MK2e-xxSBM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S12H20v9mows"},"outputs":[],"source":["# =========================================\n","# Modelo para extracción sentimiento audio\n","# =========================================\n","\n","\n","class DeepModel(nn.Module):\n","    def __init__(self, input_dim=143, output_dim=8):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","\n","            nn.Linear(64, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# =========================================\n","# rutas al modelo\n","# =========================================\n","modelo_emo= os.path.join(model_path, \"deep_model.pth\")\n","Scaler= os.path.join(model_path, \"scaler.pkl\")\n","encoder= os.path.join(model_path, \"label_encoder.pkl\")\n","\n","model_e = DeepModel(input_dim=143, output_dim=8)\n","model_e.load_state_dict(torch.load(modelo_emo))\n","model_e = model_e.to(device).to(torch.float32).eval()  # Forzar float32\n","\n","scaler = joblib.load(Scaler)\n","le = joblib.load(encoder)\n","\n","# =====================\n","# Funciones\n","# =====================\n","def pad_audio_smart(data, sr, target_sec=2.5, prev_data=None):\n","\n","    target_len = int(target_sec * sr)\n","\n","    if len(data) < target_len:\n","        pad_len = target_len - len(data)\n","        if prev_data is not None and len(prev_data) >= pad_len:\n","            # Tomar del final del audio anterior\n","            pad = prev_data[-pad_len:]\n","        else:\n","            # Rellenar con ceros\n","            pad = np.zeros(pad_len, dtype=data.dtype)\n","        data = np.concatenate([pad, data])\n","    else:\n","        # Recortar al final\n","        data = data[-target_len:]\n","\n","    return data\n","\n","\n","def extract_features_emotion(data, sample_rate):\n","    # Calcular características una sola vez\n","    rms = np.mean(librosa.feature.rms(y=data, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0], dtype=np.float32)\n","    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0], dtype=np.float32)\n","    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13), axis=1, dtype=np.float32)\n","    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate, n_fft=FRAME_LENGTH, hop_length=HOP_LENGTH), axis=1, dtype=np.float32)\n","\n","    # Crear vector final\n","    features = np.concatenate([[zcr], mfcc, [rms], mel])\n","    return features\n","\n","def predict_emotion(data, sr):\n","    features = extract_features_emotion(data, sr)\n","    x_features = np.array(features).reshape(1, -1)\n","    scaled = scaler.transform(x_features)\n","    with torch.no_grad():\n","      sample = torch.tensor(scaled, dtype=torch.float32).to(device)\n","      output = model_e(sample)\n","      pred = output.argmax(dim=1)\n","      predicted_label = le.inverse_transform([pred.item()])[0]\n","      return predicted_label\n","\n"]},{"cell_type":"markdown","source":["## Descarga del vídeo TED y extracción del audio\n","\n","En este bloque se definen dos funciones fundamentales para el procesamiento de los vídeos TED:\n","\n","### `download_video(url)`\n","Esta función permite descargar el vídeo original desde su URL (TED o YouTube) utilizando la herramienta `yt-dlp`. La descarga se realiza con los siguientes ajustes:\n","\n","- Se selecciona la mejor combinación disponible de video y audio.\n","- El archivo resultante se guarda con el identificador único del vídeo como nombre (`%(id)s.mp4`).\n","- El vídeo se guarda en formato `.mp4`, y se suprimen tanto la salida detallada como las advertencias para simplificar el flujo en notebooks.\n","\n","La función devuelve la ruta del archivo descargado y el `video_id` correspondiente.\n","\n","### `extract_audio_from_video(video_path, audio_path)`\n","Una vez descargado el vídeo, esta función extrae únicamente la **pista de audio**, utilizando `ffmpeg`. El audio se guarda en formato **WAV sin comprimir**, con los siguientes parámetros:\n","\n","- Canal único (mono): `-ac 1`\n","- Frecuencia de muestreo: definida por `SAMPLE_RATE`\n","- Codificación PCM lineal: `pcm_s16le`\n","\n","Esta función es necesaria para preparar el audio en condiciones óptimas para la posterior transcripción y análisis acústico."],"metadata":{"id":"iy5Shg18y464"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsO13UBNmowt"},"outputs":[],"source":["\n","def download_video(url):\n","    ydl_opts = {\n","        'format': 'bestvideo+bestaudio/best',\n","        'outtmpl': '%(id)s.%(ext)s',\n","        'merge_output_format': 'mp4',\n","        'quiet': True,          # Evita la mayoría de la salida\n","        'no_warnings': True,    # Oculta advertencias\n","    }\n","    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n","        info = ydl.extract_info(url, download=True)\n","        video_path = f\"{info['id']}.mp4\"\n","        return video_path, info['id']\n","\n","def extract_audio_from_video(video_path, audio_path):\n","    command = [\n","        'ffmpeg', '-i',video_path, '-vn',\n","        '-acodec', 'pcm_s16le', '-ar', str(SAMPLE_RATE),\n","        '-ac', '1', audio_path, '-y'\n","    ]\n","    subprocess.run(command, stdout=subprocess.DEVNULL,\n","                   stderr=subprocess.DEVNULL)"]},{"cell_type":"markdown","source":["## Segmentación del discurso y análisis por fragmentos\n","\n","Este bloque define el procedimiento principal para segmentar el audio del vídeo en fragmentos significativos y enriquecerlos con información textual y emocional. Está compuesto por tres funciones clave:\n","\n","### `segmentos_por_pausa_enfasis()`\n","\n","Divide el audio en segmentos dinámicos en función de dos señales acústicas:\n","- **RMS**: energía del audio.\n","- **ZCR**: tasa de cruce por cero (indicador de actividad sonora).\n","\n","Los segmentos se generan si se detecta una **pausa prolongada** (`min_pause`) o un **cambio súbito de énfasis** (basado en la variación conjunta de RMS y ZCR). Se asegura una **duración mínima** (`min_seg`) para evitar fragmentos irrelevantes.\n","\n","Cada segmento resultante incluye:\n","- Tiempos de inicio y fin.\n","- Energía media (`rms_mean`), sonoridad (`zcr_mean`).\n","- Duración de la pausa anterior (`prev_pause`).\n","\n","---\n","\n","### `transcribir_con_segmentos(model, y, sr, segmentos)`\n","\n","Dada una señal de audio (`y`) ya segmentada:\n","\n","1. Se guarda temporalmente como archivo `.wav`.\n","2. Se transcribe usando Whisper, con marcas de tiempo por palabra.\n","3. Se asocian las palabras a cada segmento en función de sus tiempos de aparición.\n","4. Se calcula:\n","   - El **texto parcial** correspondiente a cada segmento.\n","   - El número de **palabras por minuto** (`pmm`).\n","   - El **tipo** de segmento: `\"Habla\"` o `\"Pausa\"`.\n","   - La **emoción acústica** estimada (`emocion`) mediante el modelo previamente cargado.\n","\n","El resultado es una lista estructurada de segmentos enriquecidos con datos acústicos, textuales y emocionales, además del **texto completo transcrito** y el **idioma detectado** por Whisper.\n","\n","---\n","\n","### `analizar_audio(audio_path)`\n","\n","Función principal que encapsula todo el flujo anterior. A partir de la ruta de un archivo de audio:\n","\n","1. Lo segmenta mediante `segmentos_por_pausa_enfasis()`.\n","2. Transcribe y analiza cada fragmento con `transcribir_con_segmentos()`.\n","3. Devuelve un diccionario con:\n","   - Duración total del vídeo.\n","   - Transcripción completa.\n","   - Idioma detectado.\n","   - Lista detallada de segmentos analizados.\n"],"metadata":{"id":"LvhUnMSYxyS0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbvpDpnSmowt"},"outputs":[],"source":["def segmentos_por_pausa_enfasis(\n","    audio_path,\n","    sr=SAMPLE_RATE,\n","    threshold=0.01,\n","    min_pause=0.4,\n","    min_seg=1.0,\n","    min_cambio_enfasis=2\n","):\n","    import librosa\n","    import numpy as np\n","\n","    y, _ = librosa.load(audio_path, sr=sr)\n","\n","    rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n","    zcr = librosa.feature.zero_crossing_rate(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n","\n","    # Normalización para énfasis\n","    rms_norm = rms / (np.max(rms) + 1e-8)\n","    zcr_norm = zcr / (np.max(zcr) + 1e-8)\n","    combined = 0.7 * rms_norm + 0.3 * zcr_norm\n","    combined_norm = combined / (np.max(combined) + 1e-8)\n","    enfasis_levels = np.clip(np.ceil(combined_norm * EMPHASIS_LEVELS), 1, EMPHASIS_LEVELS).astype(int).tolist()\n","\n","    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=HOP_LENGTH)\n","\n","    segmentos = []\n","    start_time = times[0]\n","    pause_time = 0.0\n","    prev_enfasis = enfasis_levels[0]\n","    prev_pause = 0.0\n","\n","    for i in range(1, len(rms)):\n","        time = times[i]\n","        is_pause = rms[i] < threshold\n","        enfasis_actual = enfasis_levels[i]\n","        cambio_enfasis = abs(enfasis_actual - prev_enfasis) >= min_cambio_enfasis\n","\n","        if is_pause:\n","            pause_time += times[i] - times[i - 1]\n","        else:\n","            if pause_time >= min_pause:\n","                end_time = time\n","                if end_time - start_time >= min_seg:\n","                    start_sample = int(start_time * sr)\n","                    end_sample = int(end_time * sr)\n","\n","                    seg_rms = rms[(times >= start_time) & (times <= end_time)]\n","                    seg_zcr = zcr[(times >= start_time) & (times <= end_time)]\n","                    seg_wave = y[start_sample:end_sample]\n","\n","                    segmentos.append({\n","                        \"inicio\": round(start_time, 2),\n","                        \"fin\": round(end_time, 2),\n","                        \"rms_mean\": float(np.mean(seg_rms)),\n","                        \"zcr_mean\": float(np.mean(seg_zcr)),\n","                        \"prev_pause\": prev_pause\n","                    })\n","                start_time = end_time\n","                prev_pause = pause_time\n","                pause_time = 0.0\n","            else:\n","                if cambio_enfasis and (time - start_time >= min_seg):\n","                    end_time = time\n","                    start_sample = int(start_time * sr)\n","                    end_sample = int(end_time * sr)\n","\n","                    seg_rms = rms[(times >= start_time) & (times <= end_time)]\n","                    seg_zcr = zcr[(times >= start_time) & (times <= end_time)]\n","                    seg_wave = y[start_sample:end_sample]\n","\n","                    segmentos.append({\n","                        \"inicio\": round(start_time, 2),\n","                        \"fin\": round(end_time, 2),\n","                        \"rms_mean\": float(np.mean(seg_rms)),\n","                        \"zcr_mean\": float(np.mean(seg_zcr)),\n","                        \"rms_vector\": seg_rms.tolist(),\n","                        \"prev_pause\": prev_pause\n","                    })\n","                    start_time = end_time\n","                    prev_pause = pause_time\n","                    pause_time = 0.0\n","        prev_enfasis = enfasis_actual\n","\n","    # Último segmento\n","    if times[-1] - start_time >= min_seg:\n","        start_sample = int(start_time * sr)\n","        end_sample = len(y)\n","\n","        seg_rms = rms[(times >= start_time) & (times <= times[-1])]\n","        seg_zcr = zcr[(times >= start_time) & (times <= times[-1])]\n","        seg_wave = y[start_sample:end_sample]\n","\n","        segmentos.append({\n","            \"inicio\": round(start_time, 2),\n","            \"fin\": round(times[-1], 2),\n","            \"rms_mean\": float(np.mean(seg_rms)),\n","            \"zcr_mean\": float(np.mean(seg_zcr)),\n","            \"prev_pause\": prev_pause\n","        })\n","\n","    return y, sr, segmentos\n","\n","def transcribir_con_segmentos(model, y, sr, segmentos):\n","    segmentos_lista = []\n","    texto_completo = \"\"\n","    idioma_detectado = None\n","\n","    # Guardar todo el audio como archivo temporal\n","    with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n","        torchaudio.save(tmp.name, torch.tensor(y).unsqueeze(0), sample_rate=sr)\n","\n","        # Transcripción con Whisper\n","        whisper_gen, info = model.transcribe(tmp.name, word_timestamps=True)\n","        idioma_detectado = getattr(info, \"language\", None)\n","        whisper_segments = list(whisper_gen)\n","\n","        # Texto completo\n","        texto_completo = \" \".join([seg.text for seg in whisper_segments])\n","\n","        # Aplanar palabras\n","        todas_palabras = []\n","        for seg in whisper_segments:\n","            todas_palabras.extend(seg.words)\n","\n","        seg_id = 0\n","        for seg in segmentos:\n","            inicio = seg[\"inicio\"]\n","            fin = seg[\"fin\"]\n","            rms_mean = seg[\"rms_mean\"]\n","            zcr_mean = seg[\"zcr_mean\"]\n","            prev_pause = seg[\"prev_pause\"]\n","            seg_id += 1\n","\n","            start_sample = int(inicio * sr)\n","            end_sample = int(fin * sr)\n","            corte_audio = y[start_sample:end_sample]\n","\n","            # Palabras dentro del rango\n","            palabras_segmento = [\n","                w.word for w in todas_palabras\n","                if w.end > inicio and w.start < fin\n","            ]\n","            texto = \" \".join(palabras_segmento).strip()\n","\n","            if texto:\n","                tipo = \"Habla\"\n","                duracion_min = (fin - inicio) / 60  # sin redondear antes\n","                pmm = len(texto.split()) / duracion_min if duracion_min > 0 else 0\n","                emotion = predict_emotion(corte_audio, sr)\n","            else:\n","                tipo = \"Pausa\"\n","                pmm = 0\n","                emotion = \"\"\n","\n","            segmentos_lista.append({\n","                \"seg_id\": seg_id,\n","                \"audio\": {\n","                    \"inicio\": round(inicio, 2),\n","                    \"fin\": round(fin, 2),\n","                    \"duracion\": round(fin - inicio, 2),\n","                    \"pausa_anterior\": prev_pause,\n","                    \"rms_mean\": rms_mean,\n","                    \"zcr_mean\": zcr_mean,\n","                    \"tipo\": tipo,  # cambiado a minúscula para consistencia\n","                    \"pmm\": pmm,\n","                    \"texto\": texto,\n","                    \"emocion\": emotion\n","                },\n","                \"video\":{\n","                    \"t_central\": round ((inicio + fin) / 2,2)\n","                }\n","            })\n","\n","    return segmentos_lista, texto_completo, idioma_detectado\n","\n","def analizar_audio(audio_path):\n","    y, sr, segmentos = segmentos_por_pausa_enfasis(audio_path)\n","\n","    segmentos_lista, texto_completo,idioma = transcribir_con_segmentos(model_whisper, y, sr, segmentos)\n","\n","    duracion_video = librosa.get_duration(y=y, sr=sr)\n","\n","    return {\n","        \"duracion_video\": duracion_video,\n","        \"texto_completo\": texto_completo,\n","        \"idioma\": idioma,\n","        \"segmentos\": segmentos_lista\n","    }"]},{"cell_type":"markdown","source":["## Análisis visual de los segmentos mediante YOLO y MediaPipe\n","\n","Este bloque implementa el análisis visual por fotograma para cada segmento del vídeo, utilizando modelos ligeros para detectar postura, gestos faciales y movimiento de manos. Este proceso genera las variables visuales asociadas a cada fragmento, descritas en el apartado **3.2.3 del TFM**.\n","\n","### 1. Inicialización de modelos\n","\n","- Se carga **YOLOv8n** para detectar personas en cada fotograma.\n","- Se inicializan los modelos de **MediaPipe**:\n","  - `face_mesh`: para análisis detallado del rostro.\n","  - `pose`: para postura corporal.\n","  - `hands`: para manos y su posición.\n","\n","### 2. Detección de la persona principal (`detectar_persona_principal`)\n","\n","Dado un fotograma, se detectan todas las personas y se selecciona la más relevante en base a una puntuación que combina:\n","- **Tamaño** (área del bounding box).\n","- **Brillo** (luminosidad del rostro).\n","\n","Este recorte centrado se analiza posteriormente con MediaPipe.\n","\n","### 3. Análisis del fotograma (`analyze_frame_extended`)\n","\n","Se calcula un conjunto amplio de características visuales a partir de un solo fotograma representativo de cada segmento:\n","\n","#### 🧠 Cara:\n","- **Inclinación de la cabeza** (yaw, pitch, roll).\n","- **Boca abierta** (indicador de expresión o habla).\n","- **Sonrisa** (intensidad y detección binaria).\n","- **Ceño fruncido** (intensidad y detección binaria).\n","- **Ojos abiertos**.\n","- **Asimetría labial**.\n","- **Tensión facial** (puntuación heurística).\n","- **Estado emocional** inferido (e.g., *sonriente*, *tenso*, *sorprendido*, *neutral*).\n","\n","#### 🧍 Postura corporal:\n","- **Apertura de brazos** (medida por ángulo entre articulaciones).\n","- **Inclinación del torso** (basada en el ángulo del tronco respecto al eje vertical).\n","\n","#### ✋ Manos:\n","- **Detección de manos visibles**.\n","- **Tipo de mano** (izquierda/derecha) y **posición media**.\n","\n","### 4. Procesamiento del vídeo (`procesar_video`)\n","\n","Para cada segmento:\n","- Se calcula el **frame central** en función del tiempo.\n","- Se extrae dicho fotograma.\n","- Se aplica la detección de la persona principal.\n","- Se realiza el análisis visual completo.\n","- Los resultados se integran en el campo `\"video\"` del diccionario del segmento.\n","\n","Este análisis se repite para todos los segmentos generados durante el procesamiento del audio."],"metadata":{"id":"K9E5oXaGzdIA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7gZ8ys5mowu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757668182617,"user_tz":-120,"elapsed":385,"user":{"displayName":"Alfonso Graña","userId":"05993286614178522597"}},"outputId":"c3947837-95e5-4931-b606-7097dcfcd5f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 346.5MB/s 0.0s\n"]}],"source":["# Inicializa YOLO\n","\n","yolo_model = YOLO(\"yolov8n.pt\")\n","\n","# ---- Inicialización de Mediapipe ----\n","def init_solutions():\n","    mp_face_mesh = mp.solutions.face_mesh\n","    mp_pose = mp.solutions.pose\n","    mp_hands = mp.solutions.hands\n","    return mp_face_mesh, mp_pose, mp_hands\n","\n","# ---- Función de ángulo ----\n","def calculate_angle(a, b, c):\n","    a = np.array([a.x, a.y])\n","    b = np.array([b.x, b.y])\n","    c = np.array([c.x, c.y])\n","    ba = a - b\n","    bc = c - b\n","    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n","    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n","    return np.degrees(angle)\n","\n","# ---- Detección persona principal ----\n","def detectar_persona_principal(frame, yolo_model, conf_thresh=0.3, alpha=0.6):\n","    detections = yolo_model.predict(frame, conf=conf_thresh, verbose=False)[0]\n","    person_class_id = 0\n","    persons = [box for box in detections.boxes if int(box.cls) == person_class_id]\n","\n","    if not persons:\n","        return frame  # no hay personas detectadas\n","\n","    areas, brightness = [], []\n","    for box in persons:\n","        coords = box.xyxy.cpu().numpy().flatten()\n","        x1, y1, x2, y2 = map(int, coords)\n","        w, h = x2 - x1, y2 - y1\n","        areas.append(w * h)\n","\n","        crop = frame[y1:y2, x1:x2]\n","        if crop.size == 0:\n","            brightness.append(0)\n","        else:\n","            gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n","            brightness.append(np.mean(gray))\n","\n","    areas = np.array(areas) / (np.max(areas) + 1e-6)\n","    brightness = np.array(brightness) / (np.max(brightness) + 1e-6)\n","    scores = alpha * areas + (1 - alpha) * brightness\n","    idx = np.argmax(scores)\n","\n","    coords = persons[idx].xyxy.cpu().numpy().flatten()\n","    x1, y1, x2, y2 = map(int, coords)\n","\n","    return frame[y1:y2, x1:x2]\n","\n","# ---- Análisis de frame ----\n","def analyze_frame_extended(frame, frame_id, face_mesh, pose, hands):\n","    frame_resize = cv2.resize(frame, (320, int(frame.shape[0] * 320 / frame.shape[1])))\n","    rgb_frame = cv2.cvtColor(frame_resize, cv2.COLOR_BGR2RGB)\n","    result_dict = {\n","        \"frame_id\": frame_id,\n","        \"cara_detectada\": False,\n","        \"inclinacion_cabeza\": {\"yaw\": None, \"pitch\": None, \"roll\": None},\n","        \"boca_abierta\": None,\n","        \"sonrisa\": None,\n","        \"sonrisa_detectada\": False,\n","        \"ceño_fruncido\": None,\n","        \"ceño_detectado\": False,\n","        \"ojos_abiertos\": None,\n","        \"asimetria_labios\": None,\n","        \"tension_facial\": None,\n","        \"estado_emocional\": \"desconocido\",\n","        \"apertura_brazos\": None,\n","        \"inclinacion_torso\": None,\n","        \"manos_visibles\": False,\n","        \"detalle_manos\": []\n","    }\n","\n","    # ---- Cara ----\n","    face_results = face_mesh.process(rgb_frame)\n","    if face_results.multi_face_landmarks:\n","        face = face_results.multi_face_landmarks[0]\n","        result_dict[\"cara_detectada\"] = True\n","\n","        # Referencias\n","        left_eye = face.landmark[33]\n","        right_eye = face.landmark[263]\n","        nose_tip = face.landmark[1]\n","        chin = face.landmark[152]\n","\n","        # Distancia entre ojos para normalizar medidas\n","        eye_distance = np.sqrt(\n","            (right_eye.x - left_eye.x) ** 2 + (right_eye.y - left_eye.y) ** 2\n","        )\n","\n","        # ---- Inclinación cabeza (yaw, pitch, roll) ----\n","        roll = math.degrees(math.atan2(\n","            right_eye.y - left_eye.y,\n","            right_eye.x - left_eye.x\n","        ))\n","        pitch = math.degrees(math.atan2(\n","            chin.y - nose_tip.y,\n","            chin.x - nose_tip.x\n","        ))\n","        eye_center_x = (left_eye.x + right_eye.x) / 2.0\n","        eye_center_y = (left_eye.y + right_eye.y) / 2.0\n","        yaw = math.degrees(math.atan2(\n","            nose_tip.x - eye_center_x,\n","            nose_tip.y - eye_center_y\n","        ))\n","\n","        result_dict[\"inclinacion_cabeza\"] = {\"yaw\": yaw, \"pitch\": pitch, \"roll\": roll}\n","\n","        # ---- Boca, sonrisa, ceño, ojos ----\n","        mouth_open = abs(face.landmark[13].y - face.landmark[14].y)\n","        mouth_open_norm = mouth_open / (eye_distance + 1e-6)\n","        result_dict[\"boca_abierta\"] = mouth_open_norm\n","\n","        left_mouth, right_mouth = face.landmark[61], face.landmark[291]\n","        mouth_width = abs(right_mouth.x - left_mouth.x)\n","        mouth_height = abs(face.landmark[13].y - face.landmark[14].y)\n","        smile_ratio = mouth_width / (mouth_height + 1e-6)\n","        result_dict[\"sonrisa\"] = smile_ratio\n","        result_dict[\"sonrisa_detectada\"] = smile_ratio > 1.8\n","\n","        brow_left_inner, brow_right_inner = face.landmark[70], face.landmark[300]\n","        brow_distance = abs(brow_right_inner.x - brow_left_inner.x) / (eye_distance + 1e-6)\n","        result_dict[\"ceño_fruncido\"] = brow_distance\n","        result_dict[\"ceño_detectado\"] = brow_distance < 0.04\n","\n","        left_eye_open = abs(face.landmark[159].y - face.landmark[145].y)\n","        right_eye_open = abs(face.landmark[386].y - face.landmark[374].y)\n","        eye_open_avg = (left_eye_open + right_eye_open) / 2 / (eye_distance + 1e-6)\n","        result_dict[\"ojos_abiertos\"] = eye_open_avg\n","\n","\n","        result_dict[\"asimetria_labios\"] = abs(left_mouth.y - right_mouth.y)\n","\n","        # ---- Tensión facial ----\n","        tension_score = 0\n","        if brow_distance < 0.04: tension_score += 1\n","        if eye_open_avg > 0.06: tension_score += 1\n","        if mouth_open_norm < 0.02: tension_score += 1\n","        if smile_ratio > 2.5: tension_score += 1\n","        result_dict[\"tension_facial\"] = tension_score\n","\n","        # ---- Estado emocional ----\n","        if result_dict[\"sonrisa_detectada\"] and tension_score <= 1:\n","            estado = \"sonriente\"\n","        elif tension_score >= 3:\n","            estado = \"tenso\"\n","        elif eye_open_avg > 0.08 and mouth_open_norm > 0.08:\n","            estado = \"sorprendido\"\n","        else:\n","            estado = \"neutral\"\n","        result_dict[\"estado_emocional\"] = estado\n","\n","    # ---- Postura ----\n","    pose_results = pose.process(rgb_frame)\n","    if pose_results.pose_landmarks:\n","        lm = pose_results.pose_landmarks.landmark\n","        left_angle = calculate_angle(lm[mp.solutions.pose.PoseLandmark.LEFT_ELBOW],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_HIP])\n","        right_angle = calculate_angle(lm[mp.solutions.pose.PoseLandmark.RIGHT_ELBOW],\n","                                      lm[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER],\n","                                      lm[mp.solutions.pose.PoseLandmark.RIGHT_HIP])\n","        result_dict[\"apertura_brazos\"] = (left_angle + right_angle) / 2\n","        torso_angle = calculate_angle(lm[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_HIP],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_KNEE])\n","        result_dict[\"inclinacion_torso\"] = torso_angle\n","\n","    # ---- Manos ----\n","    hand_results = hands.process(rgb_frame)\n","    if hand_results.multi_hand_landmarks:\n","        result_dict[\"manos_visibles\"] = True\n","        hands_info = []\n","        for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n","            label = handedness.classification[0].label\n","            x_list = [lm.x for lm in hand_landmarks.landmark]\n","            y_list = [lm.y for lm in hand_landmarks.landmark]\n","            hands_info.append({\n","                \"tipo\": label,\n","                \"posicion_media\": {\"x\": np.mean(x_list), \"y\": np.mean(y_list)}\n","            })\n","        result_dict[\"detalle_manos\"] = hands_info\n","\n","    return result_dict\n","\n","# ---- Procesar video ----\n","def procesar_video(video_path, segmentos_lista):\n","    mp_face_mesh, mp_pose, mp_hands = init_solutions()\n","\n","    with mp_face_mesh.FaceMesh(static_image_mode=False, refine_landmarks=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh, \\\n","         mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5) as pose, \\\n","         mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n","\n","        cap = cv2.VideoCapture(video_path)\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","        for segmento in segmentos_lista:\n","            t_central = segmento[\"video\"][\"t_central\"]\n","            frame_id = int(t_central * fps)\n","\n","            # Mover puntero del video al frame deseado\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n","            ret, frame = cap.read()\n","            if not ret:\n","                print(f\"No se pudo leer el frame {frame_id}\")\n","                continue\n","\n","            # Detectar persona principal y analizar frame\n","            principal_frame = detectar_persona_principal(frame, yolo_model)\n","            resultado = analyze_frame_extended(principal_frame, frame_id, face_mesh, pose, hands)\n","\n","            # Guardar resultado en el segmento\n","            segmento[\"video\"][\"frame_id\"] = frame_id\n","            segmento[\"video\"][\"analisis\"] = resultado\n","\n","        cap.release()\n","\n","    return segmentos_lista  # ← Devolvemos la lista modificada\n"]},{"cell_type":"markdown","metadata":{"id":"SKIrg6Tzmowv"},"source":["# **RUN**"]},{"cell_type":"markdown","source":["## Descarga y lectura del vídeo TED\n","\n","A partir del identificador de cada vídeo (`video_id`) y su URL asociada, se descarga el archivo con `yt-dlp`. Posteriormente, se extraen por separado el audio (formato `.wav`) y los fotogramas necesarios para el análisis visual.\n","\n","Este proceso se repetirá para todos los vídeos del conjunto seleccionado, organizados en grupos por lotes."],"metadata":{"id":"7l2EN-Czx6kv"}},{"cell_type":"markdown","source":["## Carga del índice general y selección de vídeos por grupo\n","\n","En este bloque se carga el archivo `videos_analisis.csv`, que contiene la lista completa de vídeos seleccionados para el estudio, junto con su categoría (buen/mal comunicador) y el grupo de procesamiento al que pertenecen (0 a 99).\n","\n","### `extraer_lista_urls(df, grupo)`\n","\n","Esta función filtra el DataFrame por un grupo específico y devuelve una lista de tuplas con los siguientes datos por vídeo:\n","- Enlace al vídeo (`link`).\n","- Categoría (`categoria`): indica si pertenece a la clase 0 o 1.\n","- Número de grupo (`grupo`).\n","\n","Este listado permite iterar sobre los vídeos que se analizarán en el lote actual, controlando así la ejecución por partes.\n","\n","En este caso se configura el grupo inicial a analizar (`grupo_inicio = 62`) y se define cuántos grupos consecutivos se procesarán (`grupos = 9`).\n"],"metadata":{"id":"OvNbfhGVz3LZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQRsZNknmowv"},"outputs":[],"source":["datos = os.path.join(folder_path, \"videos_analisis.csv\")\n","df = pd.read_csv(datos)\n","\n","def extraer_lista_urls(df, grupo):\n","    # Filtrar por ese grupo\n","    df_filtrado = df[df[\"grupo\"] == grupo].copy()\n","\n","    # 📌 EXTRAER lista de tuplas (link, likes, views, grupo)\n","    lista_videos = list(zip(\n","        df_filtrado[\"link\"],\n","        df_filtrado[\"categoria\"],\n","        df_filtrado[\"grupo\"]\n","    ))\n","\n","    print(f\"Primeras 5 tuplas del grupo {grupo}: {lista_videos[:5]}\")\n","    return lista_videos\n","\n","# =====================\n","# Elegir el grupo de videos a analizar\n","# =====================\n","grupo_inicio = 62\n","grupos = 9"]},{"cell_type":"markdown","source":["## Procesamiento por lote y guardado de resultados\n","\n","Este bloque ejecuta el procesamiento completo de los vídeos por grupos definidos previamente, automatizando todas las etapas del pipeline descritas en el capítulo 3 del TFM.\n","\n","### `convert_json(obj)`\n","\n","Función auxiliar para convertir objetos de tipo NumPy (enteros, flotantes, booleanos, arrays) a formatos compatibles con JSON estándar. Se utiliza durante el guardado de resultados para evitar errores de serialización.\n","\n","---\n","\n","### `procesar_lista_videos(urls)`\n","\n","Procesa todos los vídeos de una lista `urls` (tuplas de enlace, categoría y grupo). Para cada vídeo:\n","\n","1. Se descarga el vídeo (`yt-dlp`) y se extrae el audio (`ffmpeg`).\n","2. Se analiza el audio:\n","   - Se segmenta dinámicamente por pausas y cambios de énfasis.\n","   - Se transcribe el texto.\n","   - Se estima la emoción acústica por segmento.\n","3. Se analiza el vídeo:\n","   - Se extrae el fotograma central de cada segmento.\n","   - Se aplican modelos ligeros para detectar rostro, gestos, postura y manos.\n","4. Se integran los resultados (audio + vídeo + texto) en un diccionario estructurado.\n","5. Se limpia cualquier archivo temporal generado (audio y vídeo).\n","\n","Se lleva registro del número de vídeos procesados, del tiempo total de ejecución y del número de vídeos exitosamente analizados.\n","\n","\n","\n","En este bloque se itera por grupos consecutivos de vídeos (desde `grupo_inicio` hasta `grupo_inicio + grupos`), procesando todos los vídeos de cada grupo mediante `procesar_lista_videos()`.\n","\n","Los resultados se guardan individualmente en un archivo JSON por grupo, en la ruta de salida (`json_path`). Cada archivo tiene el nombre:\n","\n"],"metadata":{"id":"5NUGyIiu0H0Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfUdIqAnmoww","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc70719e-0aa4-49e2-8ac5-5173548cd661"},"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras 5 tuplas del grupo 1: [('https://ted.com/talks/sir_ken_robinson_do_schools_kill_creativity', 1, 1), ('https://ted.com/talks/annie_bosler_and_don_greene_how_to_practice_effectively_for_just_about_anything', 1, 1), ('https://ted.com/talks/andrew_solomon_how_the_worst_moments_in_our_lives_make_us_who_we_are', 1, 1), ('https://ted.com/talks/roselinde_torres_what_it_takes_to_be_a_great_leader', 1, 1), ('https://ted.com/talks/john_green_the_nerd_s_guide_to_learning_everything_online', 1, 1)]\n","✅ Procesando grupo 1.\n","\n","🔄 Procesando video número 1: https://ted.com/talks/sir_ken_robinson_do_schools_kill_creativity.\n","Procesando audio\n","Procesando video\n","✅ Video 66 procesado en 125.59 segundos.\n","✅ 1 videos analizados correctamente de 1 videos procesados.\n","\n","🔄 Procesando video número 2: https://ted.com/talks/annie_bosler_and_don_greene_how_to_practice_effectively_for_just_about_anything.\n","Procesando audio\n","Procesando video\n","✅ Video 24447 procesado en 102.71 segundos.\n","✅ 2 videos analizados correctamente de 2 videos procesados.\n","\n","🔄 Procesando video número 3: https://ted.com/talks/andrew_solomon_how_the_worst_moments_in_our_lives_make_us_who_we_are.\n","Procesando audio\n","Procesando video\n","✅ Video 2005 procesado en 162.19 segundos.\n","✅ 3 videos analizados correctamente de 3 videos procesados.\n","\n","🔄 Procesando video número 4: https://ted.com/talks/roselinde_torres_what_it_takes_to_be_a_great_leader.\n","Procesando audio\n","Procesando video\n","✅ Video 1930 procesado en 69.08 segundos.\n","✅ 4 videos analizados correctamente de 4 videos procesados.\n","\n","🔄 Procesando video número 5: https://ted.com/talks/john_green_the_nerd_s_guide_to_learning_everything_online.\n","[download]  73.6% of ~ 536.79MiB at  598.67KiB/s ETA 01:12 (frag 134/181)"]},{"output_type":"stream","name":"stderr","text":["ERROR: \r[download] Got error: HTTPSConnectionPool(host='pu.tedcdn.com', port=443): Read timed out. (read timeout=20.0)\n"]},{"output_type":"stream","name":"stdout","text":["Procesando audio\n","Procesando video\n","✅ Video 2305 procesado en 513.31 segundos.\n","✅ 5 videos analizados correctamente de 5 videos procesados.\n","\n","🔄 Procesando video número 6: https://ted.com/talks/hamdi_ulukaya_the_anti_ceo_playbook.\n","Procesando audio\n","Procesando video\n","✅ Video 41225 procesado en 256.05 segundos.\n","✅ 6 videos analizados correctamente de 6 videos procesados.\n","\n","🔄 Procesando video número 7: https://ted.com/talks/rutger_bregman_poverty_isn_t_a_lack_of_character_it_s_a_lack_of_cash.\n","Procesando audio\n","Procesando video\n","✅ Video 2785 procesado en 156.62 segundos.\n","✅ 7 videos analizados correctamente de 7 videos procesados.\n","\n","🔄 Procesando video número 8: https://ted.com/talks/frank_warren_half_a_million_secrets.\n","Procesando audio\n","Procesando video\n","✅ Video 1416 procesado en 61.45 segundos.\n","✅ 8 videos analizados correctamente de 8 videos procesados.\n","\n","🔄 Procesando video número 9: https://ted.com/talks/adam_savage_my_love_letter_to_cosplay.\n","Procesando audio\n","Procesando video\n","✅ Video 2552 procesado en 234.91 segundos.\n","✅ 9 videos analizados correctamente de 9 videos procesados.\n","\n","🔄 Procesando video número 10: https://ted.com/talks/susan_david_how_to_be_your_best_self_in_times_of_crisis.\n","[download]  76.1% of ~ 939.34MiB at  827.74KiB/s ETA 03:05 (frag 351/460)"]},{"output_type":"stream","name":"stderr","text":["ERROR: \r[download] Got error: HTTPSConnectionPool(host='pu.tedcdn.com', port=443): Read timed out. (read timeout=20.0)\n"]},{"output_type":"stream","name":"stdout","text":["Procesando audio\n","Procesando video\n","✅ Video 61300 procesado en 1399.5 segundos.\n","✅ 10 videos analizados correctamente de 10 videos procesados.\n","\n","🔄 Procesando video número 11: https://ted.com/talks/adrienne_mayor_the_greek_myth_of_talos_the_first_robot.\n","Procesando audio\n","Procesando video\n","✅ Video 50986 procesado en 62.19 segundos.\n","✅ 11 videos analizados correctamente de 11 videos procesados.\n","\n","🔄 Procesando video número 12: https://ted.com/talks/larry_lagerstrom_einstein_s_miracle_year.\n","Procesando audio\n","Procesando video\n","✅ Video 2754 procesado en 123.7 segundos.\n","✅ 12 videos analizados correctamente de 12 videos procesados.\n","\n","🔄 Procesando video número 13: https://ted.com/talks/rives_if_i_controlled_the_internet.\n","Procesando audio\n","Procesando video\n","✅ Video 26 procesado en 50.79 segundos.\n","✅ 13 videos analizados correctamente de 13 videos procesados.\n","\n","🔄 Procesando video número 14: https://ted.com/talks/enrico_ramirez_ruiz_your_body_was_forged_in_the_spectacular_death_of_stars.\n","Procesando audio\n","Procesando video\n","✅ Video 53522 procesado en 217.76 segundos.\n","✅ 14 videos analizados correctamente de 14 videos procesados.\n","\n","🔄 Procesando video número 15: https://ted.com/talks/greg_gage_how_octopuses_battle_each_other.\n","Procesando audio\n","Procesando video\n","✅ Video 17713 procesado en 51.33 segundos.\n","✅ 15 videos analizados correctamente de 15 videos procesados.\n","\n","🔄 Procesando video número 16: https://ted.com/talks/beth_noveck_demand_a_more_open_source_government.\n","Procesando audio\n","Procesando video\n","✅ Video 1558 procesado en 351.84 segundos.\n","✅ 16 videos analizados correctamente de 16 videos procesados.\n","\n","🔄 Procesando video número 17: https://ted.com/talks/rei_my_mama_black_banana.\n","Procesando audio\n","Procesando video\n","✅ Video 12459 procesado en 179.96 segundos.\n","✅ 17 videos analizados correctamente de 17 videos procesados.\n","\n","🔄 Procesando video número 18: https://ted.com/talks/ming_luke_what_s_a_squillo_and_why_do_opera_singers_need_it.\n","Procesando audio\n","Procesando video\n","✅ Video 60079 procesado en 56.79 segundos.\n","✅ 18 videos analizados correctamente de 18 videos procesados.\n","\n","🔄 Procesando video número 19: https://ted.com/talks/jen_gunter_why_healthy_bones_are_about_so_much_more_than_milk.\n","Procesando audio\n","Procesando video\n","✅ Video 77211 procesado en 56.06 segundos.\n","✅ 19 videos analizados correctamente de 19 videos procesados.\n","\n","🔄 Procesando video número 20: https://ted.com/talks/camilo_ramirez_the_problem_with_the_u_s_bail_system.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: \r[download] Got error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"]},{"output_type":"stream","name":"stdout","text":["Procesando audio\n","Procesando video\n","✅ Video 66819 procesado en 52.79 segundos.\n","✅ 20 videos analizados correctamente de 20 videos procesados.\n","\n","🔄 Procesando video número 21: https://ted.com/talks/sasha_dichter_the_generosity_experiment.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: unable to download video data: HTTP Error 403: Forbidden\n"]},{"output_type":"stream","name":"stdout","text":["❌ Error procesando https://ted.com/talks/sasha_dichter_the_generosity_experiment: ERROR: unable to download video data: HTTP Error 403: Forbidden\n","\n","🔄 Procesando video número 22: https://ted.com/talks/susan_shaw_the_oil_spill_s_toxic_trade_off.\n","Procesando audio\n","Procesando video\n","✅ Video 925 procesado en 296.31 segundos.\n","✅ 21 videos analizados correctamente de 22 videos procesados.\n","\n","🔄 Procesando video número 23: https://ted.com/talks/madhumita_murgia_how_data_brokers_sell_your_identity.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: unable to download video data: HTTP Error 403: Forbidden\n"]},{"output_type":"stream","name":"stdout","text":["❌ Error procesando https://ted.com/talks/madhumita_murgia_how_data_brokers_sell_your_identity: ERROR: unable to download video data: HTTP Error 403: Forbidden\n","\n","🔄 Procesando video número 24: https://ted.com/talks/jasmine_cho_how_i_use_cookies_to_teach_history.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: unable to download video data: HTTP Error 403: Forbidden\n"]},{"output_type":"stream","name":"stdout","text":["❌ Error procesando https://ted.com/talks/jasmine_cho_how_i_use_cookies_to_teach_history: ERROR: unable to download video data: HTTP Error 403: Forbidden\n","\n","🔄 Procesando video número 25: https://ted.com/talks/bob_nease_how_to_trick_yourself_into_good_behavior.\n","Procesando audio\n","Procesando video\n","✅ Video 9467 procesado en 222.89 segundos.\n","✅ 22 videos analizados correctamente de 25 videos procesados.\n","\n","🔄 Procesando video número 26: https://ted.com/talks/maeve_higgins_why_the_good_immigrant_is_a_bad_narrative.\n","Procesando audio\n","Procesando video\n","✅ Video 41916 procesado en 200.62 segundos.\n","✅ 23 videos analizados correctamente de 26 videos procesados.\n","\n","🔄 Procesando video número 27: https://ted.com/talks/jared_hill_how_i_leapt_from_a_responsible_no_to_an_impassioned_yes.\n","Procesando audio\n","Procesando video\n","✅ Video 13015 procesado en 178.52 segundos.\n","✅ 24 videos analizados correctamente de 27 videos procesados.\n","\n","🔄 Procesando video número 28: https://ted.com/talks/ise_lyfe_we_are_not_mud.\n","Procesando audio\n","Procesando video\n","✅ Video 9474 procesado en 142.26 segundos.\n","✅ 25 videos analizados correctamente de 28 videos procesados.\n","\n","✅ Videos grupo 1 procesados en 88.75 minutos.\n"]}],"source":["def convert_json(obj):\n","    if isinstance(obj, (np.integer, np.int_, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, (np.bool_, bool)):\n","        return bool(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    return str(obj)\n","\n","def procesar_lista_videos(urls):\n","    resultados = {}\n","    tiempo_total=0\n","    n_videos_procesados=0\n","    n_videos_analizados=0\n","    for url, categoria, grupo in urls:\n","        n_videos_procesados+=1\n","        print(f\"\\n🔄 Procesando video número {n_videos_procesados}: {url}.\")\n","        inicio = time.time()\n","\n","        video_path = None\n","        audio_path = None\n","\n","        try:\n","            video_path, video_id = download_video(url)\n","            audio_path = f\"{video_id}.wav\"\n","            extract_audio_from_video(video_path, audio_path)\n","            print(\"Procesando audio\")\n","\n","            resultado_audio = analizar_audio(audio_path)\n","            print(\"Procesando video\")\n","\n","            segmentos = procesar_video(video_path,resultado_audio[\"segmentos\"])\n","\n","            resultados[video_id] = {\n","                \"video_id\": video_id,\n","                \"link\": url,\n","                \"tipo_comunicador\": categoria,\n","                \"grupo\": grupo,\n","                \"duracion_video\": resultado_audio[\"duracion_video\"],\n","                \"texto_completo\": resultado_audio[\"texto_completo\"],\n","                \"idioma\": resultado_audio[\"idioma\"],\n","                \"segmentos\": segmentos\n","            }\n","\n","            n_videos_analizados+=1\n","\n","        except Exception as e:\n","            print(f\"❌ Error procesando {url}: {e}\")\n","            continue  # Pasar al siguiente video\n","\n","        finally:\n","            # Limpiar archivos temporales\n","            if video_path and os.path.exists(video_path):\n","                os.remove(video_path)\n","            if audio_path and os.path.exists(audio_path):\n","                os.remove(audio_path)\n","            clean_up()\n","\n","\n","\n","        fin = time.time()\n","        duracion = round(fin - inicio, 2)\n","\n","        tiempo_total+=duracion\n","        print(f\"✅ Video {video_id} procesado en {duracion} segundos.\")\n","        print(f\"✅ {n_videos_analizados} videos analizados correctamente de {n_videos_procesados} videos procesados.\")\n","\n","        time.sleep(random.randint(3, 7))\n","    print(f\"\\n✅ Videos grupo {grupo} procesados en {tiempo_total/60:.2f} minutos.\")\n","    return resultados\n","\n","\n","if __name__ == \"__main__\":\n","\n","    for grupo in range(grupo_inicio, grupo_inicio+grupos):\n","        lista_videos = extraer_lista_urls(df, grupo)\n","        lista_urls = lista_videos\n","        print(f\"✅ Procesando grupo {grupo}.\")\n","        resultados_finales = procesar_lista_videos(lista_urls)\n","\n","        ruta_salida = os.path.join(json_path, f\"resultados_grupo_{grupo}.json\")\n","\n","        with open(ruta_salida, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(resultados_finales, f, indent=2, ensure_ascii=False, default=convert_json)\n","\n","        print(f\"✅ Datos guardados en 'resultados_grupo_{grupo}.json'.\")\n","\n","    print(\"Limpiando memoria y reiniciando runtime para liberar GPU...\")\n","    os._exit(0)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1fwC6Ud9te2xaMZ2z3FCbKKZ1qWt0qA29","timestamp":1755429804678}]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":8070538,"sourceId":12766468,"sourceType":"datasetVersion"},{"modelId":401631,"modelInstanceId":382069,"sourceId":474855,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2b824f5050a742ed95247bbcfd5f0ad0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c8856038aea43a7baa1efb359ee3ac0","IPY_MODEL_e33d28422d9444bab1806c480a804c76","IPY_MODEL_ea052b8f1fc246f2bd4b9ed53dc4c7b4"],"layout":"IPY_MODEL_a5f90fe2368946789b039562b7271ced"}},"7c8856038aea43a7baa1efb359ee3ac0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b6f5be8b7714e3ebdabd4bd92bb00ec","placeholder":"​","style":"IPY_MODEL_c1fb0fe7573d44f6bd39afadfca053e7","value":"tokenizer.json: "}},"e33d28422d9444bab1806c480a804c76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f75bd02f6e704d2192a02b225e4826a5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab90f0b229574fcda8457ee7edc41ffe","value":1}},"ea052b8f1fc246f2bd4b9ed53dc4c7b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c388c285f434486a9b006e60265ae915","placeholder":"​","style":"IPY_MODEL_15184d71dead470eb609f059ab393450","value":" 2.20M/? [00:00&lt;00:00, 43.3MB/s]"}},"a5f90fe2368946789b039562b7271ced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6f5be8b7714e3ebdabd4bd92bb00ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1fb0fe7573d44f6bd39afadfca053e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f75bd02f6e704d2192a02b225e4826a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ab90f0b229574fcda8457ee7edc41ffe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c388c285f434486a9b006e60265ae915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15184d71dead470eb609f059ab393450":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d88320745e3144aeb3bb37ef615f32d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9460d1b1f84d4bb49fb4d5e9f0ee5d1b","IPY_MODEL_1db6cef6b5dd49b6bf31dbcd5abe4e87","IPY_MODEL_5078a2a45b554b6c8b20374b948fd514"],"layout":"IPY_MODEL_5774bbf3b3a34ac185c974f0bded1644"}},"9460d1b1f84d4bb49fb4d5e9f0ee5d1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36e1469c10294847a7b6e525c3b6160b","placeholder":"​","style":"IPY_MODEL_13cbe7d2b6364adc81ce3c1f35183038","value":"vocabulary.txt: "}},"1db6cef6b5dd49b6bf31dbcd5abe4e87":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11ea737390184d6bafde270b2ae774e3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c308a69a798424794fdc6089233dd48","value":1}},"5078a2a45b554b6c8b20374b948fd514":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f89a3cccbc69436497fc0c3d4e977c19","placeholder":"​","style":"IPY_MODEL_3ee42cb92b7b4c85aa9ca5a92f52727b","value":" 460k/? [00:00&lt;00:00, 13.9MB/s]"}},"5774bbf3b3a34ac185c974f0bded1644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36e1469c10294847a7b6e525c3b6160b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13cbe7d2b6364adc81ce3c1f35183038":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ea737390184d6bafde270b2ae774e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3c308a69a798424794fdc6089233dd48":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f89a3cccbc69436497fc0c3d4e977c19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ee42cb92b7b4c85aa9ca5a92f52727b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d518605723d44bf19b7a8c0e87ee25f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ffddf6122ce4d7886f15a2ee628b371","IPY_MODEL_2dab8b99c7e2484c835a1c3cdd509317","IPY_MODEL_6c00a587f52141cfbcf6c7f0b4e2b289"],"layout":"IPY_MODEL_d5d2643e7e814e428d0a168842cfdc97"}},"0ffddf6122ce4d7886f15a2ee628b371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b82ad0fc4bd43c19dab6fa65f52d43c","placeholder":"​","style":"IPY_MODEL_d90476cf0de3475ca52c98e2bc8dd604","value":"config.json: "}},"2dab8b99c7e2484c835a1c3cdd509317":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b79aa47d0bc8474ab288af4758d2080c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c524f727588442a97e99f4de1b89cdb","value":1}},"6c00a587f52141cfbcf6c7f0b4e2b289":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_583f267b419c4c6b986fb47b55300bf4","placeholder":"​","style":"IPY_MODEL_e217b7c8c5254f258c8e2259e8f68782","value":" 2.37k/? [00:00&lt;00:00, 200kB/s]"}},"d5d2643e7e814e428d0a168842cfdc97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b82ad0fc4bd43c19dab6fa65f52d43c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90476cf0de3475ca52c98e2bc8dd604":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b79aa47d0bc8474ab288af4758d2080c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6c524f727588442a97e99f4de1b89cdb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"583f267b419c4c6b986fb47b55300bf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e217b7c8c5254f258c8e2259e8f68782":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"832dbe7906194125ac982be3254548e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9d61acaa48b469ab7c6668ac4d0c435","IPY_MODEL_5b532466f07d4d55bd305ba717d2e09b","IPY_MODEL_b425279d57d54ac79089cc6ae41f90ac"],"layout":"IPY_MODEL_28e767265f284cd4bfc391664ea75aa6"}},"d9d61acaa48b469ab7c6668ac4d0c435":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c4ad304280d4698a841c1ad533d7cc4","placeholder":"​","style":"IPY_MODEL_f940f82a28ef423ca7a4928848d60f69","value":"model.bin: 100%"}},"5b532466f07d4d55bd305ba717d2e09b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_637666b8c0fb45249199f4a72e9d8050","max":483546902,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15d3547b70804bd69a8d71129e4a5c33","value":483546902}},"b425279d57d54ac79089cc6ae41f90ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29060677ad8e4c4db5c67df81f84cbd0","placeholder":"​","style":"IPY_MODEL_c9f4b545053d4b1c8f1153051684351b","value":" 484M/484M [00:01&lt;00:00, 531MB/s]"}},"28e767265f284cd4bfc391664ea75aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c4ad304280d4698a841c1ad533d7cc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f940f82a28ef423ca7a4928848d60f69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"637666b8c0fb45249199f4a72e9d8050":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d3547b70804bd69a8d71129e4a5c33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29060677ad8e4c4db5c67df81f84cbd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f4b545053d4b1c8f1153051684351b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}