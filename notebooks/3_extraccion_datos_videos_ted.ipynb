{"cells":[{"cell_type":"markdown","source":["# ExtracciÃ³n y procesamiento de datos multimodales en vÃ­deos TED\n","\n","Este notebook implementa el pipeline completo descrito en el apartado **3.2 del TFM**, correspondiente al **procesamiento de vÃ­deos y construcciÃ³n del dataset multimodal**.\n","\n","Incluye tareas de descarga, transcripciÃ³n automÃ¡tica, segmentaciÃ³n dinÃ¡mica, extracciÃ³n de caracterÃ­sticas acÃºsticas y visuales, y organizaciÃ³n de los resultados por vÃ­deo y por lote.\n","\n","Los datos procesados se almacenan en formato estructurado (JSON), y posteriormente integrados en estructuras tabulares para su anÃ¡lisis.\n"],"metadata":{"id":"VAvbu5QUrVeQ"}},{"cell_type":"markdown","source":["âš ï¸ Requisitos importantes antes de ejecutar este notebook\n","Este notebook estÃ¡ diseÃ±ado para ejecutarse en Google Colab.\n","Requiere el uso de una GPU T4 y una correcta configuraciÃ³n de rutas para funcionar correctamente.\n","\n","1. Tipo de entorno\n","Ve a Entorno de ejecuciÃ³n > Cambiar tipo de entorno y selecciona GPU T4\n","Comprueba que se ha asignado una GPU T4 ejecutando:\n","2. InstalaciÃ³n de librerÃ­as\n","Tras la instalaciÃ³n de las librerÃ­as necesarias, es obligatorio reiniciar el entorno antes de continuar con la ejecuciÃ³n.\n","\n","Ve a Entorno de ejecuciÃ³n > Reiniciar entorno de ejecuciÃ³n\n","\n","3. ConfiguraciÃ³n de rutas\n","Antes de ejecutar las celdas principales, configura correctamente las siguientes rutas:\n","\n","folder_path: Carpeta de trabajo principal.\n","\n","json_path: Carpeta donde descargaremos los json, a meddida que se vayan analizando los videos.\n","\n","model_path: Carpeta donde estÃ¡ guardado el modelo de emociones audio\n"],"metadata":{"id":"M0DuBG2xw8Jd"}},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"id":"at_Z2Stvmown","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1757870274551,"user_tz":-120,"elapsed":21003,"user":{"displayName":"Alfonso GraÃ±a","userId":"05993286614178522597"}},"outputId":"9bf8693b-9782-4214-eb0d-04d2192cb736"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faster-whisper\n","  Downloading faster_whisper-1.2.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting ultralytics\n","  Downloading ultralytics-8.3.199-py3-none-any.whl.metadata (37 kB)\n","Collecting yt-dlp\n","  Downloading yt_dlp-2025.9.5-py3-none-any.whl.metadata (177 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n","  Downloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.34.4)\n","Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.0)\n","Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n","  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting av>=11 (from faster-whisper)\n","  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.67.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n","Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio) (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.19.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.4.0)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.9)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchaudio) (3.0.2)\n","Downloading faster_whisper-1.2.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics-8.3.199-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yt_dlp-2025.9.5-py3-none-any.whl (3.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n","Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: yt-dlp, protobuf, humanfriendly, ctranslate2, av, sounddevice, coloredlogs, onnxruntime, ultralytics-thop, mediapipe, faster-whisper, ultralytics\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed av-15.1.0 coloredlogs-15.0.1 ctranslate2-4.6.0 faster-whisper-1.2.0 humanfriendly-10.0 mediapipe-0.10.14 onnxruntime-1.22.1 protobuf-4.25.8 sounddevice-0.5.2 ultralytics-8.3.199 ultralytics-thop-2.0.17 yt-dlp-2025.9.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"ec01847c3f744c6f82e9cf791aca15db"}},"metadata":{}}],"source":["!pip install faster-whisper librosa torchaudio ultralytics yt-dlp opencv-python pandas matplotlib mediapipe"]},{"cell_type":"markdown","metadata":{"id":"_HHMvIUmmowq"},"source":["# **IMPORTACIONES**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bjKxdh6kmowq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757870295115,"user_tz":-120,"elapsed":10541,"user":{"displayName":"Alfonso GraÃ±a","userId":"05993286614178522597"}},"outputId":"658af561-bcaa-43eb-8030-a1b1290d4b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["import os\n","import warnings\n","\n","# ---- Ignorar warnings de Python ----\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","import torch\n","import torchaudio\n","import librosa\n","import joblib\n","import pandas as pd\n","from ultralytics import YOLO\n","import mediapipe as mp\n","\n","\n","import time\n","import json\n","import numpy as np\n","import subprocess\n","import tempfile\n","from faster_whisper import WhisperModel\n","import yt_dlp\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import math\n","import random\n","import cv2\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## ConfiguraciÃ³n del entorno y rutas de trabajo\n","\n","Se definen las rutas de entrada y salida donde se almacenarÃ¡n los vÃ­deos descargados, las transcripciones, las caracterÃ­sticas extraÃ­das y los resultados finales.\n","\n","TambiÃ©n se configuran los parÃ¡metros globales del procesamiento de audio (frecuencia de muestreo, tamaÃ±o de ventana, etc.).\n"],"metadata":{"id":"V64cleYTxHnz"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"ulX4mqYlnMJm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757870304131,"user_tz":-120,"elapsed":1449,"user":{"displayName":"Alfonso GraÃ±a","userId":"05993286614178522597"}},"outputId":"5479f03f-9232-4267-e859-33fecde039d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#carpeta de trabajo principal\n","folder_path = \"/content/drive/MyDrive/Analisis_Multimodal_Comunicacion_TFM/data/folder_path\"\n","#carpeta para guardar los archivos conforme se vallan analizando videos\n","json_path = \"/content/drive/MyDrive/Analisis_Multimodal_Comunicacion_TFM/data/json_path\"\n","#carpeta donde estÃ¡ el modelo de emodiones del audio\n","model_path = \"/content/drive/MyDrive/Analisis_Multimodal_Comunicacion_TFM/models\"\n","\n","\n","\n","os.makedirs(folder_path, exist_ok=True)\n","os.makedirs(json_path, exist_ok=True)\n","os.makedirs(model_path, exist_ok=True)"]},{"cell_type":"markdown","source":["## ConfiguraciÃ³n del dispositivo y carga del modelo Whisper\n","\n","Se detecta automÃ¡ticamente si hay una GPU disponible para acelerar el procesamiento. Luego se inicializa el modelo Whisper (`small`) para realizar la transcripciÃ³n automÃ¡tica de los vÃ­deos, con soporte para mÃºltiples idiomas.\n","\n","Este modelo se usarÃ¡ mÃ¡s adelante para obtener la transcripciÃ³n cronometrada de cada vÃ­deo, paso fundamental para la segmentaciÃ³n dinÃ¡mica y anÃ¡lisis textual.\n"],"metadata":{"id":"aA1Wa_mxxPC8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgQICUxbmowr","colab":{"base_uri":"https://localhost:8080/","height":327,"referenced_widgets":["2b824f5050a742ed95247bbcfd5f0ad0","7c8856038aea43a7baa1efb359ee3ac0","e33d28422d9444bab1806c480a804c76","ea052b8f1fc246f2bd4b9ed53dc4c7b4","a5f90fe2368946789b039562b7271ced","2b6f5be8b7714e3ebdabd4bd92bb00ec","c1fb0fe7573d44f6bd39afadfca053e7","f75bd02f6e704d2192a02b225e4826a5","ab90f0b229574fcda8457ee7edc41ffe","c388c285f434486a9b006e60265ae915","15184d71dead470eb609f059ab393450","d88320745e3144aeb3bb37ef615f32d8","9460d1b1f84d4bb49fb4d5e9f0ee5d1b","1db6cef6b5dd49b6bf31dbcd5abe4e87","5078a2a45b554b6c8b20374b948fd514","5774bbf3b3a34ac185c974f0bded1644","36e1469c10294847a7b6e525c3b6160b","13cbe7d2b6364adc81ce3c1f35183038","11ea737390184d6bafde270b2ae774e3","3c308a69a798424794fdc6089233dd48","f89a3cccbc69436497fc0c3d4e977c19","3ee42cb92b7b4c85aa9ca5a92f52727b","d518605723d44bf19b7a8c0e87ee25f4","0ffddf6122ce4d7886f15a2ee628b371","2dab8b99c7e2484c835a1c3cdd509317","6c00a587f52141cfbcf6c7f0b4e2b289","d5d2643e7e814e428d0a168842cfdc97","0b82ad0fc4bd43c19dab6fa65f52d43c","d90476cf0de3475ca52c98e2bc8dd604","b79aa47d0bc8474ab288af4758d2080c","6c524f727588442a97e99f4de1b89cdb","583f267b419c4c6b986fb47b55300bf4","e217b7c8c5254f258c8e2259e8f68782","832dbe7906194125ac982be3254548e3","d9d61acaa48b469ab7c6668ac4d0c435","5b532466f07d4d55bd305ba717d2e09b","b425279d57d54ac79089cc6ae41f90ac","28e767265f284cd4bfc391664ea75aa6","6c4ad304280d4698a841c1ad533d7cc4","f940f82a28ef423ca7a4928848d60f69","637666b8c0fb45249199f4a72e9d8050","15d3547b70804bd69a8d71129e4a5c33","29060677ad8e4c4db5c67df81f84cbd0","c9f4b545053d4b1c8f1153051684351b"]},"executionInfo":{"status":"ok","timestamp":1757668179142,"user_tz":-120,"elapsed":4681,"user":{"displayName":"Alfonso GraÃ±a","userId":"05993286614178522597"}},"outputId":"78328fdc-e1bf-4874-cf2c-c98140ab6b49"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","ConfiguraciÃ³n de Dispositivo:\n","Tipo: CUDA\n","GPU: NVIDIA A100-SXM4-80GB\n","Capacidad: (8, 0)\n","Memoria Total: 79.32 GB\n","==================================================\n","\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b824f5050a742ed95247bbcfd5f0ad0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocabulary.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88320745e3144aeb3bb37ef615f32d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d518605723d44bf19b7a8c0e87ee25f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.bin:   0%|          | 0.00/484M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832dbe7906194125ac982be3254548e3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Modelo Whisper configurado en cuda con compute_type=float32\n"]}],"source":["# =====================\n","# Config\n","# =====================\n","SAMPLE_RATE = 16000\n","FRAME_LENGTH = 2048\n","HOP_LENGTH = 512\n","EMPHASIS_LEVELS = 10\n","\n","# =====================\n","# Definir device\n","# =====================\n","\n","# ConfiguraciÃ³n inicial\n","device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(device_type)\n","\n","# VerificaciÃ³n detallada del dispositivo\n","print(f\"\\n{'='*50}\")\n","print(f\"ConfiguraciÃ³n de Dispositivo:\")\n","print(f\"Tipo: {device_type.upper()}\")\n","if device_type == \"cuda\":\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Capacidad: {torch.cuda.get_device_capability()}\")\n","    print(f\"Memoria Total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n","print(f\"{'='*50}\\n\")\n","\n","# Usar float32 por defecto para mayor estabilidad\n","torch.set_default_dtype(torch.float32)\n","compute_type = \"float32\"  # Para Whisper\n","\n","# =========================================\n","# Definir modelo para extracciÃ³n del texto\n","# =========================================\n","\n","model_whisper = WhisperModel(\n","    \"small\",\n","    device=device_type,\n","    compute_type=compute_type,\n",")\n","print(f\"Modelo Whisper configurado en {device_type} con compute_type={compute_type}\")\n","\n","\n","def clean_up():\n","    if device_type == \"cuda\":\n","        torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","source":["  ## Modelo para la detecciÃ³n de emociones acÃºsticas\n","\n","En este bloque se carga y aplica el modelo entrenado para predecir la emociÃ³n predominante en un segmento de audio, como se describe en el apartado **3.2.5 del TFM**.\n","\n","### Arquitectura del modelo\n","\n","Se define una red neuronal profunda (`DeepModel`) implementada con PyTorch. Su entrada es un vector de 143 caracterÃ­sticas acÃºsticas, y su salida corresponde a una de las 8 emociones posibles (segÃºn el dataset RAVDESS). La arquitectura incluye:\n","\n","- Capas totalmente conectadas (`Linear`) con activaciÃ³n ReLU.\n","- RegularizaciÃ³n mediante `Dropout` en varias capas ocultas.\n","- Una capa de salida con tamaÃ±o igual al nÃºmero de clases emocionales.\n","\n","El modelo se carga desde disco (`deep_model.pth`) y se pasa a modo evaluaciÃ³n (`eval()`), utilizando `float32` para mantener la estabilidad numÃ©rica.\n","\n","### NormalizaciÃ³n y codificaciÃ³n\n","\n","Se cargan tambiÃ©n dos objetos entrenados previamente:\n","- Un **`StandardScaler`** (`scaler.pkl`) para normalizar las caracterÃ­sticas antes de la predicciÃ³n.\n","- Un **`LabelEncoder`** (`label_encoder.pkl`) para decodificar las predicciones numÃ©ricas en etiquetas emocionales (por ejemplo, *alegrÃ­a*, *tristeza*, etc.).\n","\n","### Funciones auxiliares\n","\n","Se definen dos funciones clave:\n","\n","- `pad_audio_smart()`: ajusta cualquier segmento de audio a una longitud fija (2.5 segundos) mediante recorte o padding (con ceros o parte del audio anterior), para asegurar una entrada consistente al modelo.\n","\n","- `extract_features_emotion()`: calcula y concatena las caracterÃ­sticas acÃºsticas que alimentan al modelo:\n","  - ZCR (Zero Crossing Rate)\n","  - MFCC (13 coeficientes)\n","  - RMS (energÃ­a)\n","  - Mel Spectrogram (resumen por bandas)\n","\n","- `predict_emotion()`: transforma el audio en caracterÃ­sticas, las normaliza, las pasa por el modelo y devuelve la **emociÃ³n estimada como etiqueta** (texto).\n","\n","Esta funciÃ³n se aplicarÃ¡ a cada segmento de audio extraÃ­do del corpus TED, generando el valor `emocion_audio` que se"],"metadata":{"id":"76MK2e-xxSBM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S12H20v9mows"},"outputs":[],"source":["# =========================================\n","# Modelo para extracciÃ³n sentimiento audio\n","# =========================================\n","\n","\n","class DeepModel(nn.Module):\n","    def __init__(self, input_dim=143, output_dim=8):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","\n","            nn.Linear(64, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# =========================================\n","# rutas al modelo\n","# =========================================\n","modelo_emo= os.path.join(model_path, \"deep_model.pth\")\n","Scaler= os.path.join(model_path, \"scaler.pkl\")\n","encoder= os.path.join(model_path, \"label_encoder.pkl\")\n","\n","model_e = DeepModel(input_dim=143, output_dim=8)\n","model_e.load_state_dict(torch.load(modelo_emo))\n","model_e = model_e.to(device).to(torch.float32).eval()  # Forzar float32\n","\n","scaler = joblib.load(Scaler)\n","le = joblib.load(encoder)\n","\n","# =====================\n","# Funciones\n","# =====================\n","def pad_audio_smart(data, sr, target_sec=2.5, prev_data=None):\n","\n","    target_len = int(target_sec * sr)\n","\n","    if len(data) < target_len:\n","        pad_len = target_len - len(data)\n","        if prev_data is not None and len(prev_data) >= pad_len:\n","            # Tomar del final del audio anterior\n","            pad = prev_data[-pad_len:]\n","        else:\n","            # Rellenar con ceros\n","            pad = np.zeros(pad_len, dtype=data.dtype)\n","        data = np.concatenate([pad, data])\n","    else:\n","        # Recortar al final\n","        data = data[-target_len:]\n","\n","    return data\n","\n","\n","def extract_features_emotion(data, sample_rate):\n","    # Calcular caracterÃ­sticas una sola vez\n","    rms = np.mean(librosa.feature.rms(y=data, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0], dtype=np.float32)\n","    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0], dtype=np.float32)\n","    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13), axis=1, dtype=np.float32)\n","    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate, n_fft=FRAME_LENGTH, hop_length=HOP_LENGTH), axis=1, dtype=np.float32)\n","\n","    # Crear vector final\n","    features = np.concatenate([[zcr], mfcc, [rms], mel])\n","    return features\n","\n","def predict_emotion(data, sr):\n","    features = extract_features_emotion(data, sr)\n","    x_features = np.array(features).reshape(1, -1)\n","    scaled = scaler.transform(x_features)\n","    with torch.no_grad():\n","      sample = torch.tensor(scaled, dtype=torch.float32).to(device)\n","      output = model_e(sample)\n","      pred = output.argmax(dim=1)\n","      predicted_label = le.inverse_transform([pred.item()])[0]\n","      return predicted_label\n","\n"]},{"cell_type":"markdown","source":["## Descarga del vÃ­deo TED y extracciÃ³n del audio\n","\n","En este bloque se definen dos funciones fundamentales para el procesamiento de los vÃ­deos TED:\n","\n","### `download_video(url)`\n","Esta funciÃ³n permite descargar el vÃ­deo original desde su URL (TED o YouTube) utilizando la herramienta `yt-dlp`. La descarga se realiza con los siguientes ajustes:\n","\n","- Se selecciona la mejor combinaciÃ³n disponible de video y audio.\n","- El archivo resultante se guarda con el identificador Ãºnico del vÃ­deo como nombre (`%(id)s.mp4`).\n","- El vÃ­deo se guarda en formato `.mp4`, y se suprimen tanto la salida detallada como las advertencias para simplificar el flujo en notebooks.\n","\n","La funciÃ³n devuelve la ruta del archivo descargado y el `video_id` correspondiente.\n","\n","### `extract_audio_from_video(video_path, audio_path)`\n","Una vez descargado el vÃ­deo, esta funciÃ³n extrae Ãºnicamente la **pista de audio**, utilizando `ffmpeg`. El audio se guarda en formato **WAV sin comprimir**, con los siguientes parÃ¡metros:\n","\n","- Canal Ãºnico (mono): `-ac 1`\n","- Frecuencia de muestreo: definida por `SAMPLE_RATE`\n","- CodificaciÃ³n PCM lineal: `pcm_s16le`\n","\n","Esta funciÃ³n es necesaria para preparar el audio en condiciones Ã³ptimas para la posterior transcripciÃ³n y anÃ¡lisis acÃºstico."],"metadata":{"id":"iy5Shg18y464"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsO13UBNmowt"},"outputs":[],"source":["\n","def download_video(url):\n","    ydl_opts = {\n","        'format': 'bestvideo+bestaudio/best',\n","        'outtmpl': '%(id)s.%(ext)s',\n","        'merge_output_format': 'mp4',\n","        'quiet': True,          # Evita la mayorÃ­a de la salida\n","        'no_warnings': True,    # Oculta advertencias\n","    }\n","    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n","        info = ydl.extract_info(url, download=True)\n","        video_path = f\"{info['id']}.mp4\"\n","        return video_path, info['id']\n","\n","def extract_audio_from_video(video_path, audio_path):\n","    command = [\n","        'ffmpeg', '-i',video_path, '-vn',\n","        '-acodec', 'pcm_s16le', '-ar', str(SAMPLE_RATE),\n","        '-ac', '1', audio_path, '-y'\n","    ]\n","    subprocess.run(command, stdout=subprocess.DEVNULL,\n","                   stderr=subprocess.DEVNULL)"]},{"cell_type":"markdown","source":["## SegmentaciÃ³n del discurso y anÃ¡lisis por fragmentos\n","\n","Este bloque define el procedimiento principal para segmentar el audio del vÃ­deo en fragmentos significativos y enriquecerlos con informaciÃ³n textual y emocional. EstÃ¡ compuesto por tres funciones clave:\n","\n","### `segmentos_por_pausa_enfasis()`\n","\n","Divide el audio en segmentos dinÃ¡micos en funciÃ³n de dos seÃ±ales acÃºsticas:\n","- **RMS**: energÃ­a del audio.\n","- **ZCR**: tasa de cruce por cero (indicador de actividad sonora).\n","\n","Los segmentos se generan si se detecta una **pausa prolongada** (`min_pause`) o un **cambio sÃºbito de Ã©nfasis** (basado en la variaciÃ³n conjunta de RMS y ZCR). Se asegura una **duraciÃ³n mÃ­nima** (`min_seg`) para evitar fragmentos irrelevantes.\n","\n","Cada segmento resultante incluye:\n","- Tiempos de inicio y fin.\n","- EnergÃ­a media (`rms_mean`), sonoridad (`zcr_mean`).\n","- DuraciÃ³n de la pausa anterior (`prev_pause`).\n","\n","---\n","\n","### `transcribir_con_segmentos(model, y, sr, segmentos)`\n","\n","Dada una seÃ±al de audio (`y`) ya segmentada:\n","\n","1. Se guarda temporalmente como archivo `.wav`.\n","2. Se transcribe usando Whisper, con marcas de tiempo por palabra.\n","3. Se asocian las palabras a cada segmento en funciÃ³n de sus tiempos de apariciÃ³n.\n","4. Se calcula:\n","   - El **texto parcial** correspondiente a cada segmento.\n","   - El nÃºmero de **palabras por minuto** (`pmm`).\n","   - El **tipo** de segmento: `\"Habla\"` o `\"Pausa\"`.\n","   - La **emociÃ³n acÃºstica** estimada (`emocion`) mediante el modelo previamente cargado.\n","\n","El resultado es una lista estructurada de segmentos enriquecidos con datos acÃºsticos, textuales y emocionales, ademÃ¡s del **texto completo transcrito** y el **idioma detectado** por Whisper.\n","\n","---\n","\n","### `analizar_audio(audio_path)`\n","\n","FunciÃ³n principal que encapsula todo el flujo anterior. A partir de la ruta de un archivo de audio:\n","\n","1. Lo segmenta mediante `segmentos_por_pausa_enfasis()`.\n","2. Transcribe y analiza cada fragmento con `transcribir_con_segmentos()`.\n","3. Devuelve un diccionario con:\n","   - DuraciÃ³n total del vÃ­deo.\n","   - TranscripciÃ³n completa.\n","   - Idioma detectado.\n","   - Lista detallada de segmentos analizados.\n"],"metadata":{"id":"LvhUnMSYxyS0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbvpDpnSmowt"},"outputs":[],"source":["def segmentos_por_pausa_enfasis(\n","    audio_path,\n","    sr=SAMPLE_RATE,\n","    threshold=0.01,\n","    min_pause=0.4,\n","    min_seg=1.0,\n","    min_cambio_enfasis=2\n","):\n","    import librosa\n","    import numpy as np\n","\n","    y, _ = librosa.load(audio_path, sr=sr)\n","\n","    rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n","    zcr = librosa.feature.zero_crossing_rate(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n","\n","    # NormalizaciÃ³n para Ã©nfasis\n","    rms_norm = rms / (np.max(rms) + 1e-8)\n","    zcr_norm = zcr / (np.max(zcr) + 1e-8)\n","    combined = 0.7 * rms_norm + 0.3 * zcr_norm\n","    combined_norm = combined / (np.max(combined) + 1e-8)\n","    enfasis_levels = np.clip(np.ceil(combined_norm * EMPHASIS_LEVELS), 1, EMPHASIS_LEVELS).astype(int).tolist()\n","\n","    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=HOP_LENGTH)\n","\n","    segmentos = []\n","    start_time = times[0]\n","    pause_time = 0.0\n","    prev_enfasis = enfasis_levels[0]\n","    prev_pause = 0.0\n","\n","    for i in range(1, len(rms)):\n","        time = times[i]\n","        is_pause = rms[i] < threshold\n","        enfasis_actual = enfasis_levels[i]\n","        cambio_enfasis = abs(enfasis_actual - prev_enfasis) >= min_cambio_enfasis\n","\n","        if is_pause:\n","            pause_time += times[i] - times[i - 1]\n","        else:\n","            if pause_time >= min_pause:\n","                end_time = time\n","                if end_time - start_time >= min_seg:\n","                    start_sample = int(start_time * sr)\n","                    end_sample = int(end_time * sr)\n","\n","                    seg_rms = rms[(times >= start_time) & (times <= end_time)]\n","                    seg_zcr = zcr[(times >= start_time) & (times <= end_time)]\n","                    seg_wave = y[start_sample:end_sample]\n","\n","                    segmentos.append({\n","                        \"inicio\": round(start_time, 2),\n","                        \"fin\": round(end_time, 2),\n","                        \"rms_mean\": float(np.mean(seg_rms)),\n","                        \"zcr_mean\": float(np.mean(seg_zcr)),\n","                        \"prev_pause\": prev_pause\n","                    })\n","                start_time = end_time\n","                prev_pause = pause_time\n","                pause_time = 0.0\n","            else:\n","                if cambio_enfasis and (time - start_time >= min_seg):\n","                    end_time = time\n","                    start_sample = int(start_time * sr)\n","                    end_sample = int(end_time * sr)\n","\n","                    seg_rms = rms[(times >= start_time) & (times <= end_time)]\n","                    seg_zcr = zcr[(times >= start_time) & (times <= end_time)]\n","                    seg_wave = y[start_sample:end_sample]\n","\n","                    segmentos.append({\n","                        \"inicio\": round(start_time, 2),\n","                        \"fin\": round(end_time, 2),\n","                        \"rms_mean\": float(np.mean(seg_rms)),\n","                        \"zcr_mean\": float(np.mean(seg_zcr)),\n","                        \"rms_vector\": seg_rms.tolist(),\n","                        \"prev_pause\": prev_pause\n","                    })\n","                    start_time = end_time\n","                    prev_pause = pause_time\n","                    pause_time = 0.0\n","        prev_enfasis = enfasis_actual\n","\n","    # Ãšltimo segmento\n","    if times[-1] - start_time >= min_seg:\n","        start_sample = int(start_time * sr)\n","        end_sample = len(y)\n","\n","        seg_rms = rms[(times >= start_time) & (times <= times[-1])]\n","        seg_zcr = zcr[(times >= start_time) & (times <= times[-1])]\n","        seg_wave = y[start_sample:end_sample]\n","\n","        segmentos.append({\n","            \"inicio\": round(start_time, 2),\n","            \"fin\": round(times[-1], 2),\n","            \"rms_mean\": float(np.mean(seg_rms)),\n","            \"zcr_mean\": float(np.mean(seg_zcr)),\n","            \"prev_pause\": prev_pause\n","        })\n","\n","    return y, sr, segmentos\n","\n","def transcribir_con_segmentos(model, y, sr, segmentos):\n","    segmentos_lista = []\n","    texto_completo = \"\"\n","    idioma_detectado = None\n","\n","    # Guardar todo el audio como archivo temporal\n","    with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n","        torchaudio.save(tmp.name, torch.tensor(y).unsqueeze(0), sample_rate=sr)\n","\n","        # TranscripciÃ³n con Whisper\n","        whisper_gen, info = model.transcribe(tmp.name, word_timestamps=True)\n","        idioma_detectado = getattr(info, \"language\", None)\n","        whisper_segments = list(whisper_gen)\n","\n","        # Texto completo\n","        texto_completo = \" \".join([seg.text for seg in whisper_segments])\n","\n","        # Aplanar palabras\n","        todas_palabras = []\n","        for seg in whisper_segments:\n","            todas_palabras.extend(seg.words)\n","\n","        seg_id = 0\n","        for seg in segmentos:\n","            inicio = seg[\"inicio\"]\n","            fin = seg[\"fin\"]\n","            rms_mean = seg[\"rms_mean\"]\n","            zcr_mean = seg[\"zcr_mean\"]\n","            prev_pause = seg[\"prev_pause\"]\n","            seg_id += 1\n","\n","            start_sample = int(inicio * sr)\n","            end_sample = int(fin * sr)\n","            corte_audio = y[start_sample:end_sample]\n","\n","            # Palabras dentro del rango\n","            palabras_segmento = [\n","                w.word for w in todas_palabras\n","                if w.end > inicio and w.start < fin\n","            ]\n","            texto = \" \".join(palabras_segmento).strip()\n","\n","            if texto:\n","                tipo = \"Habla\"\n","                duracion_min = (fin - inicio) / 60  # sin redondear antes\n","                pmm = len(texto.split()) / duracion_min if duracion_min > 0 else 0\n","                emotion = predict_emotion(corte_audio, sr)\n","            else:\n","                tipo = \"Pausa\"\n","                pmm = 0\n","                emotion = \"\"\n","\n","            segmentos_lista.append({\n","                \"seg_id\": seg_id,\n","                \"audio\": {\n","                    \"inicio\": round(inicio, 2),\n","                    \"fin\": round(fin, 2),\n","                    \"duracion\": round(fin - inicio, 2),\n","                    \"pausa_anterior\": prev_pause,\n","                    \"rms_mean\": rms_mean,\n","                    \"zcr_mean\": zcr_mean,\n","                    \"tipo\": tipo,  # cambiado a minÃºscula para consistencia\n","                    \"pmm\": pmm,\n","                    \"texto\": texto,\n","                    \"emocion\": emotion\n","                },\n","                \"video\":{\n","                    \"t_central\": round ((inicio + fin) / 2,2)\n","                }\n","            })\n","\n","    return segmentos_lista, texto_completo, idioma_detectado\n","\n","def analizar_audio(audio_path):\n","    y, sr, segmentos = segmentos_por_pausa_enfasis(audio_path)\n","\n","    segmentos_lista, texto_completo,idioma = transcribir_con_segmentos(model_whisper, y, sr, segmentos)\n","\n","    duracion_video = librosa.get_duration(y=y, sr=sr)\n","\n","    return {\n","        \"duracion_video\": duracion_video,\n","        \"texto_completo\": texto_completo,\n","        \"idioma\": idioma,\n","        \"segmentos\": segmentos_lista\n","    }"]},{"cell_type":"markdown","source":["## AnÃ¡lisis visual de los segmentos mediante YOLO y MediaPipe\n","\n","Este bloque implementa el anÃ¡lisis visual por fotograma para cada segmento del vÃ­deo, utilizando modelos ligeros para detectar postura, gestos faciales y movimiento de manos. Este proceso genera las variables visuales asociadas a cada fragmento, descritas en el apartado **3.2.3 del TFM**.\n","\n","### 1. InicializaciÃ³n de modelos\n","\n","- Se carga **YOLOv8n** para detectar personas en cada fotograma.\n","- Se inicializan los modelos de **MediaPipe**:\n","  - `face_mesh`: para anÃ¡lisis detallado del rostro.\n","  - `pose`: para postura corporal.\n","  - `hands`: para manos y su posiciÃ³n.\n","\n","### 2. DetecciÃ³n de la persona principal (`detectar_persona_principal`)\n","\n","Dado un fotograma, se detectan todas las personas y se selecciona la mÃ¡s relevante en base a una puntuaciÃ³n que combina:\n","- **TamaÃ±o** (Ã¡rea del bounding box).\n","- **Brillo** (luminosidad del rostro).\n","\n","Este recorte centrado se analiza posteriormente con MediaPipe.\n","\n","### 3. AnÃ¡lisis del fotograma (`analyze_frame_extended`)\n","\n","Se calcula un conjunto amplio de caracterÃ­sticas visuales a partir de un solo fotograma representativo de cada segmento:\n","\n","#### ğŸ§  Cara:\n","- **InclinaciÃ³n de la cabeza** (yaw, pitch, roll).\n","- **Boca abierta** (indicador de expresiÃ³n o habla).\n","- **Sonrisa** (intensidad y detecciÃ³n binaria).\n","- **CeÃ±o fruncido** (intensidad y detecciÃ³n binaria).\n","- **Ojos abiertos**.\n","- **AsimetrÃ­a labial**.\n","- **TensiÃ³n facial** (puntuaciÃ³n heurÃ­stica).\n","- **Estado emocional** inferido (e.g., *sonriente*, *tenso*, *sorprendido*, *neutral*).\n","\n","#### ğŸ§ Postura corporal:\n","- **Apertura de brazos** (medida por Ã¡ngulo entre articulaciones).\n","- **InclinaciÃ³n del torso** (basada en el Ã¡ngulo del tronco respecto al eje vertical).\n","\n","#### âœ‹ Manos:\n","- **DetecciÃ³n de manos visibles**.\n","- **Tipo de mano** (izquierda/derecha) y **posiciÃ³n media**.\n","\n","### 4. Procesamiento del vÃ­deo (`procesar_video`)\n","\n","Para cada segmento:\n","- Se calcula el **frame central** en funciÃ³n del tiempo.\n","- Se extrae dicho fotograma.\n","- Se aplica la detecciÃ³n de la persona principal.\n","- Se realiza el anÃ¡lisis visual completo.\n","- Los resultados se integran en el campo `\"video\"` del diccionario del segmento.\n","\n","Este anÃ¡lisis se repite para todos los segmentos generados durante el procesamiento del audio."],"metadata":{"id":"K9E5oXaGzdIA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7gZ8ys5mowu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757668182617,"user_tz":-120,"elapsed":385,"user":{"displayName":"Alfonso GraÃ±a","userId":"05993286614178522597"}},"outputId":"c3947837-95e5-4931-b606-7097dcfcd5f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 346.5MB/s 0.0s\n"]}],"source":["# Inicializa YOLO\n","\n","yolo_model = YOLO(\"yolov8n.pt\")\n","\n","# ---- InicializaciÃ³n de Mediapipe ----\n","def init_solutions():\n","    mp_face_mesh = mp.solutions.face_mesh\n","    mp_pose = mp.solutions.pose\n","    mp_hands = mp.solutions.hands\n","    return mp_face_mesh, mp_pose, mp_hands\n","\n","# ---- FunciÃ³n de Ã¡ngulo ----\n","def calculate_angle(a, b, c):\n","    a = np.array([a.x, a.y])\n","    b = np.array([b.x, b.y])\n","    c = np.array([c.x, c.y])\n","    ba = a - b\n","    bc = c - b\n","    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n","    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n","    return np.degrees(angle)\n","\n","# ---- DetecciÃ³n persona principal ----\n","def detectar_persona_principal(frame, yolo_model, conf_thresh=0.3, alpha=0.6):\n","    detections = yolo_model.predict(frame, conf=conf_thresh, verbose=False)[0]\n","    person_class_id = 0\n","    persons = [box for box in detections.boxes if int(box.cls) == person_class_id]\n","\n","    if not persons:\n","        return frame  # no hay personas detectadas\n","\n","    areas, brightness = [], []\n","    for box in persons:\n","        coords = box.xyxy.cpu().numpy().flatten()\n","        x1, y1, x2, y2 = map(int, coords)\n","        w, h = x2 - x1, y2 - y1\n","        areas.append(w * h)\n","\n","        crop = frame[y1:y2, x1:x2]\n","        if crop.size == 0:\n","            brightness.append(0)\n","        else:\n","            gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n","            brightness.append(np.mean(gray))\n","\n","    areas = np.array(areas) / (np.max(areas) + 1e-6)\n","    brightness = np.array(brightness) / (np.max(brightness) + 1e-6)\n","    scores = alpha * areas + (1 - alpha) * brightness\n","    idx = np.argmax(scores)\n","\n","    coords = persons[idx].xyxy.cpu().numpy().flatten()\n","    x1, y1, x2, y2 = map(int, coords)\n","\n","    return frame[y1:y2, x1:x2]\n","\n","# ---- AnÃ¡lisis de frame ----\n","def analyze_frame_extended(frame, frame_id, face_mesh, pose, hands):\n","    frame_resize = cv2.resize(frame, (320, int(frame.shape[0] * 320 / frame.shape[1])))\n","    rgb_frame = cv2.cvtColor(frame_resize, cv2.COLOR_BGR2RGB)\n","    result_dict = {\n","        \"frame_id\": frame_id,\n","        \"cara_detectada\": False,\n","        \"inclinacion_cabeza\": {\"yaw\": None, \"pitch\": None, \"roll\": None},\n","        \"boca_abierta\": None,\n","        \"sonrisa\": None,\n","        \"sonrisa_detectada\": False,\n","        \"ceÃ±o_fruncido\": None,\n","        \"ceÃ±o_detectado\": False,\n","        \"ojos_abiertos\": None,\n","        \"asimetria_labios\": None,\n","        \"tension_facial\": None,\n","        \"estado_emocional\": \"desconocido\",\n","        \"apertura_brazos\": None,\n","        \"inclinacion_torso\": None,\n","        \"manos_visibles\": False,\n","        \"detalle_manos\": []\n","    }\n","\n","    # ---- Cara ----\n","    face_results = face_mesh.process(rgb_frame)\n","    if face_results.multi_face_landmarks:\n","        face = face_results.multi_face_landmarks[0]\n","        result_dict[\"cara_detectada\"] = True\n","\n","        # Referencias\n","        left_eye = face.landmark[33]\n","        right_eye = face.landmark[263]\n","        nose_tip = face.landmark[1]\n","        chin = face.landmark[152]\n","\n","        # Distancia entre ojos para normalizar medidas\n","        eye_distance = np.sqrt(\n","            (right_eye.x - left_eye.x) ** 2 + (right_eye.y - left_eye.y) ** 2\n","        )\n","\n","        # ---- InclinaciÃ³n cabeza (yaw, pitch, roll) ----\n","        roll = math.degrees(math.atan2(\n","            right_eye.y - left_eye.y,\n","            right_eye.x - left_eye.x\n","        ))\n","        pitch = math.degrees(math.atan2(\n","            chin.y - nose_tip.y,\n","            chin.x - nose_tip.x\n","        ))\n","        eye_center_x = (left_eye.x + right_eye.x) / 2.0\n","        eye_center_y = (left_eye.y + right_eye.y) / 2.0\n","        yaw = math.degrees(math.atan2(\n","            nose_tip.x - eye_center_x,\n","            nose_tip.y - eye_center_y\n","        ))\n","\n","        result_dict[\"inclinacion_cabeza\"] = {\"yaw\": yaw, \"pitch\": pitch, \"roll\": roll}\n","\n","        # ---- Boca, sonrisa, ceÃ±o, ojos ----\n","        mouth_open = abs(face.landmark[13].y - face.landmark[14].y)\n","        mouth_open_norm = mouth_open / (eye_distance + 1e-6)\n","        result_dict[\"boca_abierta\"] = mouth_open_norm\n","\n","        left_mouth, right_mouth = face.landmark[61], face.landmark[291]\n","        mouth_width = abs(right_mouth.x - left_mouth.x)\n","        mouth_height = abs(face.landmark[13].y - face.landmark[14].y)\n","        smile_ratio = mouth_width / (mouth_height + 1e-6)\n","        result_dict[\"sonrisa\"] = smile_ratio\n","        result_dict[\"sonrisa_detectada\"] = smile_ratio > 1.8\n","\n","        brow_left_inner, brow_right_inner = face.landmark[70], face.landmark[300]\n","        brow_distance = abs(brow_right_inner.x - brow_left_inner.x) / (eye_distance + 1e-6)\n","        result_dict[\"ceÃ±o_fruncido\"] = brow_distance\n","        result_dict[\"ceÃ±o_detectado\"] = brow_distance < 0.04\n","\n","        left_eye_open = abs(face.landmark[159].y - face.landmark[145].y)\n","        right_eye_open = abs(face.landmark[386].y - face.landmark[374].y)\n","        eye_open_avg = (left_eye_open + right_eye_open) / 2 / (eye_distance + 1e-6)\n","        result_dict[\"ojos_abiertos\"] = eye_open_avg\n","\n","\n","        result_dict[\"asimetria_labios\"] = abs(left_mouth.y - right_mouth.y)\n","\n","        # ---- TensiÃ³n facial ----\n","        tension_score = 0\n","        if brow_distance < 0.04: tension_score += 1\n","        if eye_open_avg > 0.06: tension_score += 1\n","        if mouth_open_norm < 0.02: tension_score += 1\n","        if smile_ratio > 2.5: tension_score += 1\n","        result_dict[\"tension_facial\"] = tension_score\n","\n","        # ---- Estado emocional ----\n","        if result_dict[\"sonrisa_detectada\"] and tension_score <= 1:\n","            estado = \"sonriente\"\n","        elif tension_score >= 3:\n","            estado = \"tenso\"\n","        elif eye_open_avg > 0.08 and mouth_open_norm > 0.08:\n","            estado = \"sorprendido\"\n","        else:\n","            estado = \"neutral\"\n","        result_dict[\"estado_emocional\"] = estado\n","\n","    # ---- Postura ----\n","    pose_results = pose.process(rgb_frame)\n","    if pose_results.pose_landmarks:\n","        lm = pose_results.pose_landmarks.landmark\n","        left_angle = calculate_angle(lm[mp.solutions.pose.PoseLandmark.LEFT_ELBOW],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_HIP])\n","        right_angle = calculate_angle(lm[mp.solutions.pose.PoseLandmark.RIGHT_ELBOW],\n","                                      lm[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER],\n","                                      lm[mp.solutions.pose.PoseLandmark.RIGHT_HIP])\n","        result_dict[\"apertura_brazos\"] = (left_angle + right_angle) / 2\n","        torso_angle = calculate_angle(lm[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_HIP],\n","                                     lm[mp.solutions.pose.PoseLandmark.LEFT_KNEE])\n","        result_dict[\"inclinacion_torso\"] = torso_angle\n","\n","    # ---- Manos ----\n","    hand_results = hands.process(rgb_frame)\n","    if hand_results.multi_hand_landmarks:\n","        result_dict[\"manos_visibles\"] = True\n","        hands_info = []\n","        for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks, hand_results.multi_handedness):\n","            label = handedness.classification[0].label\n","            x_list = [lm.x for lm in hand_landmarks.landmark]\n","            y_list = [lm.y for lm in hand_landmarks.landmark]\n","            hands_info.append({\n","                \"tipo\": label,\n","                \"posicion_media\": {\"x\": np.mean(x_list), \"y\": np.mean(y_list)}\n","            })\n","        result_dict[\"detalle_manos\"] = hands_info\n","\n","    return result_dict\n","\n","# ---- Procesar video ----\n","def procesar_video(video_path, segmentos_lista):\n","    mp_face_mesh, mp_pose, mp_hands = init_solutions()\n","\n","    with mp_face_mesh.FaceMesh(static_image_mode=False, refine_landmarks=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh, \\\n","         mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5) as pose, \\\n","         mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands:\n","\n","        cap = cv2.VideoCapture(video_path)\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","        for segmento in segmentos_lista:\n","            t_central = segmento[\"video\"][\"t_central\"]\n","            frame_id = int(t_central * fps)\n","\n","            # Mover puntero del video al frame deseado\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n","            ret, frame = cap.read()\n","            if not ret:\n","                print(f\"No se pudo leer el frame {frame_id}\")\n","                continue\n","\n","            # Detectar persona principal y analizar frame\n","            principal_frame = detectar_persona_principal(frame, yolo_model)\n","            resultado = analyze_frame_extended(principal_frame, frame_id, face_mesh, pose, hands)\n","\n","            # Guardar resultado en el segmento\n","            segmento[\"video\"][\"frame_id\"] = frame_id\n","            segmento[\"video\"][\"analisis\"] = resultado\n","\n","        cap.release()\n","\n","    return segmentos_lista  # â† Devolvemos la lista modificada\n"]},{"cell_type":"markdown","metadata":{"id":"SKIrg6Tzmowv"},"source":["# **RUN**"]},{"cell_type":"markdown","source":["## Descarga y lectura del vÃ­deo TED\n","\n","A partir del identificador de cada vÃ­deo (`video_id`) y su URL asociada, se descarga el archivo con `yt-dlp`. Posteriormente, se extraen por separado el audio (formato `.wav`) y los fotogramas necesarios para el anÃ¡lisis visual.\n","\n","Este proceso se repetirÃ¡ para todos los vÃ­deos del conjunto seleccionado, organizados en grupos por lotes."],"metadata":{"id":"7l2EN-Czx6kv"}},{"cell_type":"markdown","source":["## Carga del Ã­ndice general y selecciÃ³n de vÃ­deos por grupo\n","\n","En este bloque se carga el archivo `videos_analisis.csv`, que contiene la lista completa de vÃ­deos seleccionados para el estudio, junto con su categorÃ­a (buen/mal comunicador) y el grupo de procesamiento al que pertenecen (0 a 99).\n","\n","### `extraer_lista_urls(df, grupo)`\n","\n","Esta funciÃ³n filtra el DataFrame por un grupo especÃ­fico y devuelve una lista de tuplas con los siguientes datos por vÃ­deo:\n","- Enlace al vÃ­deo (`link`).\n","- CategorÃ­a (`categoria`): indica si pertenece a la clase 0 o 1.\n","- NÃºmero de grupo (`grupo`).\n","\n","Este listado permite iterar sobre los vÃ­deos que se analizarÃ¡n en el lote actual, controlando asÃ­ la ejecuciÃ³n por partes.\n","\n","En este caso se configura el grupo inicial a analizar (`grupo_inicio = 62`) y se define cuÃ¡ntos grupos consecutivos se procesarÃ¡n (`grupos = 9`).\n"],"metadata":{"id":"OvNbfhGVz3LZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQRsZNknmowv"},"outputs":[],"source":["datos = os.path.join(folder_path, \"videos_analisis.csv\")\n","df = pd.read_csv(datos)\n","\n","def extraer_lista_urls(df, grupo):\n","    # Filtrar por ese grupo\n","    df_filtrado = df[df[\"grupo\"] == grupo].copy()\n","\n","    # ğŸ“Œ EXTRAER lista de tuplas (link, likes, views, grupo)\n","    lista_videos = list(zip(\n","        df_filtrado[\"link\"],\n","        df_filtrado[\"categoria\"],\n","        df_filtrado[\"grupo\"]\n","    ))\n","\n","    print(f\"Primeras 5 tuplas del grupo {grupo}: {lista_videos[:5]}\")\n","    return lista_videos\n","\n","# =====================\n","# Elegir el grupo de videos a analizar\n","# =====================\n","grupo_inicio = 62\n","grupos = 9"]},{"cell_type":"markdown","source":["## Procesamiento por lote y guardado de resultados\n","\n","Este bloque ejecuta el procesamiento completo de los vÃ­deos por grupos definidos previamente, automatizando todas las etapas del pipeline descritas en el capÃ­tulo 3 del TFM.\n","\n","### `convert_json(obj)`\n","\n","FunciÃ³n auxiliar para convertir objetos de tipo NumPy (enteros, flotantes, booleanos, arrays) a formatos compatibles con JSON estÃ¡ndar. Se utiliza durante el guardado de resultados para evitar errores de serializaciÃ³n.\n","\n","---\n","\n","### `procesar_lista_videos(urls)`\n","\n","Procesa todos los vÃ­deos de una lista `urls` (tuplas de enlace, categorÃ­a y grupo). Para cada vÃ­deo:\n","\n","1. Se descarga el vÃ­deo (`yt-dlp`) y se extrae el audio (`ffmpeg`).\n","2. Se analiza el audio:\n","   - Se segmenta dinÃ¡micamente por pausas y cambios de Ã©nfasis.\n","   - Se transcribe el texto.\n","   - Se estima la emociÃ³n acÃºstica por segmento.\n","3. Se analiza el vÃ­deo:\n","   - Se extrae el fotograma central de cada segmento.\n","   - Se aplican modelos ligeros para detectar rostro, gestos, postura y manos.\n","4. Se integran los resultados (audio + vÃ­deo + texto) en un diccionario estructurado.\n","5. Se limpia cualquier archivo temporal generado (audio y vÃ­deo).\n","\n","Se lleva registro del nÃºmero de vÃ­deos procesados, del tiempo total de ejecuciÃ³n y del nÃºmero de vÃ­deos exitosamente analizados.\n","\n","\n","\n","En este bloque se itera por grupos consecutivos de vÃ­deos (desde `grupo_inicio` hasta `grupo_inicio + grupos`), procesando todos los vÃ­deos de cada grupo mediante `procesar_lista_videos()`.\n","\n","Los resultados se guardan individualmente en un archivo JSON por grupo, en la ruta de salida (`json_path`). Cada archivo tiene el nombre:\n","\n"],"metadata":{"id":"5NUGyIiu0H0Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfUdIqAnmoww","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc70719e-0aa4-49e2-8ac5-5173548cd661"},"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras 5 tuplas del grupo 1: [('https://ted.com/talks/sir_ken_robinson_do_schools_kill_creativity', 1, 1), ('https://ted.com/talks/annie_bosler_and_don_greene_how_to_practice_effectively_for_just_about_anything', 1, 1), ('https://ted.com/talks/andrew_solomon_how_the_worst_moments_in_our_lives_make_us_who_we_are', 1, 1), ('https://ted.com/talks/roselinde_torres_what_it_takes_to_be_a_great_leader', 1, 1), ('https://ted.com/talks/john_green_the_nerd_s_guide_to_learning_everything_online', 1, 1)]\n","âœ… Procesando grupo 1.\n","\n","ğŸ”„ Procesando video nÃºmero 1: https://ted.com/talks/sir_ken_robinson_do_schools_kill_creativity.\n","Procesando audio\n","Procesando video\n","âœ… Video 66 procesado en 125.59 segundos.\n","âœ… 1 videos analizados correctamente de 1 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 2: https://ted.com/talks/annie_bosler_and_don_greene_how_to_practice_effectively_for_just_about_anything.\n","Procesando audio\n","Procesando video\n","âœ… Video 24447 procesado en 102.71 segundos.\n","âœ… 2 videos analizados correctamente de 2 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 3: https://ted.com/talks/andrew_solomon_how_the_worst_moments_in_our_lives_make_us_who_we_are.\n","Procesando audio\n","Procesando video\n","âœ… Video 2005 procesado en 162.19 segundos.\n","âœ… 3 videos analizados correctamente de 3 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 4: https://ted.com/talks/roselinde_torres_what_it_takes_to_be_a_great_leader.\n","Procesando audio\n","Procesando video\n","âœ… Video 1930 procesado en 69.08 segundos.\n","âœ… 4 videos analizados correctamente de 4 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 5: https://ted.com/talks/john_green_the_nerd_s_guide_to_learning_everything_online.\n","[download]  73.6% of ~ 536.79MiB at  598.67KiB/s ETA 01:12 (frag 134/181)"]},{"output_type":"stream","name":"stderr","text":["ERROR: \r[download] Got error: HTTPSConnectionPool(host='pu.tedcdn.com', port=443): Read timed out. (read timeout=20.0)\n"]},{"output_type":"stream","name":"stdout","text":["Procesando audio\n","Procesando video\n","âœ… Video 2305 procesado en 513.31 segundos.\n","âœ… 5 videos analizados correctamente de 5 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 6: https://ted.com/talks/hamdi_ulukaya_the_anti_ceo_playbook.\n","Procesando audio\n","Procesando video\n","âœ… Video 41225 procesado en 256.05 segundos.\n","âœ… 6 videos analizados correctamente de 6 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 7: https://ted.com/talks/rutger_bregman_poverty_isn_t_a_lack_of_character_it_s_a_lack_of_cash.\n","Procesando audio\n","Procesando video\n","âœ… Video 2785 procesado en 156.62 segundos.\n","âœ… 7 videos analizados correctamente de 7 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 8: https://ted.com/talks/frank_warren_half_a_million_secrets.\n","Procesando audio\n","Procesando video\n","âœ… Video 1416 procesado en 61.45 segundos.\n","âœ… 8 videos analizados correctamente de 8 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 9: https://ted.com/talks/adam_savage_my_love_letter_to_cosplay.\n","Procesando audio\n","Procesando video\n","âœ… Video 2552 procesado en 234.91 segundos.\n","âœ… 9 videos analizados correctamente de 9 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 10: https://ted.com/talks/susan_david_how_to_be_your_best_self_in_times_of_crisis.\n","[download]  76.1% of ~ 939.34MiB at  827.74KiB/s ETA 03:05 (frag 351/460)"]},{"output_type":"stream","name":"stderr","text":["ERROR: \r[download] Got error: HTTPSConnectionPool(host='pu.tedcdn.com', port=443): Read timed out. (read timeout=20.0)\n"]},{"output_type":"stream","name":"stdout","text":["Procesando audio\n","Procesando video\n","âœ… Video 61300 procesado en 1399.5 segundos.\n","âœ… 10 videos analizados correctamente de 10 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 11: https://ted.com/talks/adrienne_mayor_the_greek_myth_of_talos_the_first_robot.\n","Procesando audio\n","Procesando video\n","âœ… Video 50986 procesado en 62.19 segundos.\n","âœ… 11 videos analizados correctamente de 11 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 12: https://ted.com/talks/larry_lagerstrom_einstein_s_miracle_year.\n","Procesando audio\n","Procesando video\n","âœ… Video 2754 procesado en 123.7 segundos.\n","âœ… 12 videos analizados correctamente de 12 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 13: https://ted.com/talks/rives_if_i_controlled_the_internet.\n","Procesando audio\n","Procesando video\n","âœ… Video 26 procesado en 50.79 segundos.\n","âœ… 13 videos analizados correctamente de 13 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 14: https://ted.com/talks/enrico_ramirez_ruiz_your_body_was_forged_in_the_spectacular_death_of_stars.\n","Procesando audio\n","Procesando video\n","âœ… Video 53522 procesado en 217.76 segundos.\n","âœ… 14 videos analizados correctamente de 14 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 15: https://ted.com/talks/greg_gage_how_octopuses_battle_each_other.\n","Procesando audio\n","Procesando video\n","âœ… Video 17713 procesado en 51.33 segundos.\n","âœ… 15 videos analizados correctamente de 15 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 16: https://ted.com/talks/beth_noveck_demand_a_more_open_source_government.\n","Procesando audio\n","Procesando video\n","âœ… Video 1558 procesado en 351.84 segundos.\n","âœ… 16 videos analizados correctamente de 16 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 17: https://ted.com/talks/rei_my_mama_black_banana.\n","Procesando audio\n","Procesando video\n","âœ… Video 12459 procesado en 179.96 segundos.\n","âœ… 17 videos analizados correctamente de 17 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 18: https://ted.com/talks/ming_luke_what_s_a_squillo_and_why_do_opera_singers_need_it.\n","Procesando audio\n","Procesando video\n","âœ… Video 60079 procesado en 56.79 segundos.\n","âœ… 18 videos analizados correctamente de 18 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 19: https://ted.com/talks/jen_gunter_why_healthy_bones_are_about_so_much_more_than_milk.\n","Procesando audio\n","Procesando video\n","âœ… Video 77211 procesado en 56.06 segundos.\n","âœ… 19 videos analizados correctamente de 19 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 20: https://ted.com/talks/camilo_ramirez_the_problem_with_the_u_s_bail_system.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: \r[download] Got error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"]},{"output_type":"stream","name":"stdout","text":["Procesando audio\n","Procesando video\n","âœ… Video 66819 procesado en 52.79 segundos.\n","âœ… 20 videos analizados correctamente de 20 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 21: https://ted.com/talks/sasha_dichter_the_generosity_experiment.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: unable to download video data: HTTP Error 403: Forbidden\n"]},{"output_type":"stream","name":"stdout","text":["âŒ Error procesando https://ted.com/talks/sasha_dichter_the_generosity_experiment: ERROR: unable to download video data: HTTP Error 403: Forbidden\n","\n","ğŸ”„ Procesando video nÃºmero 22: https://ted.com/talks/susan_shaw_the_oil_spill_s_toxic_trade_off.\n","Procesando audio\n","Procesando video\n","âœ… Video 925 procesado en 296.31 segundos.\n","âœ… 21 videos analizados correctamente de 22 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 23: https://ted.com/talks/madhumita_murgia_how_data_brokers_sell_your_identity.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: unable to download video data: HTTP Error 403: Forbidden\n"]},{"output_type":"stream","name":"stdout","text":["âŒ Error procesando https://ted.com/talks/madhumita_murgia_how_data_brokers_sell_your_identity: ERROR: unable to download video data: HTTP Error 403: Forbidden\n","\n","ğŸ”„ Procesando video nÃºmero 24: https://ted.com/talks/jasmine_cho_how_i_use_cookies_to_teach_history.\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: unable to download video data: HTTP Error 403: Forbidden\n"]},{"output_type":"stream","name":"stdout","text":["âŒ Error procesando https://ted.com/talks/jasmine_cho_how_i_use_cookies_to_teach_history: ERROR: unable to download video data: HTTP Error 403: Forbidden\n","\n","ğŸ”„ Procesando video nÃºmero 25: https://ted.com/talks/bob_nease_how_to_trick_yourself_into_good_behavior.\n","Procesando audio\n","Procesando video\n","âœ… Video 9467 procesado en 222.89 segundos.\n","âœ… 22 videos analizados correctamente de 25 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 26: https://ted.com/talks/maeve_higgins_why_the_good_immigrant_is_a_bad_narrative.\n","Procesando audio\n","Procesando video\n","âœ… Video 41916 procesado en 200.62 segundos.\n","âœ… 23 videos analizados correctamente de 26 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 27: https://ted.com/talks/jared_hill_how_i_leapt_from_a_responsible_no_to_an_impassioned_yes.\n","Procesando audio\n","Procesando video\n","âœ… Video 13015 procesado en 178.52 segundos.\n","âœ… 24 videos analizados correctamente de 27 videos procesados.\n","\n","ğŸ”„ Procesando video nÃºmero 28: https://ted.com/talks/ise_lyfe_we_are_not_mud.\n","Procesando audio\n","Procesando video\n","âœ… Video 9474 procesado en 142.26 segundos.\n","âœ… 25 videos analizados correctamente de 28 videos procesados.\n","\n","âœ… Videos grupo 1 procesados en 88.75 minutos.\n"]}],"source":["def convert_json(obj):\n","    if isinstance(obj, (np.integer, np.int_, np.int64)):\n","        return int(obj)\n","    elif isinstance(obj, (np.floating, np.float64)):\n","        return float(obj)\n","    elif isinstance(obj, (np.bool_, bool)):\n","        return bool(obj)\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    return str(obj)\n","\n","def procesar_lista_videos(urls):\n","    resultados = {}\n","    tiempo_total=0\n","    n_videos_procesados=0\n","    n_videos_analizados=0\n","    for url, categoria, grupo in urls:\n","        n_videos_procesados+=1\n","        print(f\"\\nğŸ”„ Procesando video nÃºmero {n_videos_procesados}: {url}.\")\n","        inicio = time.time()\n","\n","        video_path = None\n","        audio_path = None\n","\n","        try:\n","            video_path, video_id = download_video(url)\n","            audio_path = f\"{video_id}.wav\"\n","            extract_audio_from_video(video_path, audio_path)\n","            print(\"Procesando audio\")\n","\n","            resultado_audio = analizar_audio(audio_path)\n","            print(\"Procesando video\")\n","\n","            segmentos = procesar_video(video_path,resultado_audio[\"segmentos\"])\n","\n","            resultados[video_id] = {\n","                \"video_id\": video_id,\n","                \"link\": url,\n","                \"tipo_comunicador\": categoria,\n","                \"grupo\": grupo,\n","                \"duracion_video\": resultado_audio[\"duracion_video\"],\n","                \"texto_completo\": resultado_audio[\"texto_completo\"],\n","                \"idioma\": resultado_audio[\"idioma\"],\n","                \"segmentos\": segmentos\n","            }\n","\n","            n_videos_analizados+=1\n","\n","        except Exception as e:\n","            print(f\"âŒ Error procesando {url}: {e}\")\n","            continue  # Pasar al siguiente video\n","\n","        finally:\n","            # Limpiar archivos temporales\n","            if video_path and os.path.exists(video_path):\n","                os.remove(video_path)\n","            if audio_path and os.path.exists(audio_path):\n","                os.remove(audio_path)\n","            clean_up()\n","\n","\n","\n","        fin = time.time()\n","        duracion = round(fin - inicio, 2)\n","\n","        tiempo_total+=duracion\n","        print(f\"âœ… Video {video_id} procesado en {duracion} segundos.\")\n","        print(f\"âœ… {n_videos_analizados} videos analizados correctamente de {n_videos_procesados} videos procesados.\")\n","\n","        time.sleep(random.randint(3, 7))\n","    print(f\"\\nâœ… Videos grupo {grupo} procesados en {tiempo_total/60:.2f} minutos.\")\n","    return resultados\n","\n","\n","if __name__ == \"__main__\":\n","\n","    for grupo in range(grupo_inicio, grupo_inicio+grupos):\n","        lista_videos = extraer_lista_urls(df, grupo)\n","        lista_urls = lista_videos\n","        print(f\"âœ… Procesando grupo {grupo}.\")\n","        resultados_finales = procesar_lista_videos(lista_urls)\n","\n","        ruta_salida = os.path.join(json_path, f\"resultados_grupo_{grupo}.json\")\n","\n","        with open(ruta_salida, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(resultados_finales, f, indent=2, ensure_ascii=False, default=convert_json)\n","\n","        print(f\"âœ… Datos guardados en 'resultados_grupo_{grupo}.json'.\")\n","\n","    print(\"Limpiando memoria y reiniciando runtime para liberar GPU...\")\n","    os._exit(0)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1fwC6Ud9te2xaMZ2z3FCbKKZ1qWt0qA29","timestamp":1755429804678}]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":8070538,"sourceId":12766468,"sourceType":"datasetVersion"},{"modelId":401631,"modelInstanceId":382069,"sourceId":474855,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2b824f5050a742ed95247bbcfd5f0ad0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c8856038aea43a7baa1efb359ee3ac0","IPY_MODEL_e33d28422d9444bab1806c480a804c76","IPY_MODEL_ea052b8f1fc246f2bd4b9ed53dc4c7b4"],"layout":"IPY_MODEL_a5f90fe2368946789b039562b7271ced"}},"7c8856038aea43a7baa1efb359ee3ac0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b6f5be8b7714e3ebdabd4bd92bb00ec","placeholder":"â€‹","style":"IPY_MODEL_c1fb0fe7573d44f6bd39afadfca053e7","value":"tokenizer.json:â€‡"}},"e33d28422d9444bab1806c480a804c76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f75bd02f6e704d2192a02b225e4826a5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab90f0b229574fcda8457ee7edc41ffe","value":1}},"ea052b8f1fc246f2bd4b9ed53dc4c7b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c388c285f434486a9b006e60265ae915","placeholder":"â€‹","style":"IPY_MODEL_15184d71dead470eb609f059ab393450","value":"â€‡2.20M/?â€‡[00:00&lt;00:00,â€‡43.3MB/s]"}},"a5f90fe2368946789b039562b7271ced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6f5be8b7714e3ebdabd4bd92bb00ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1fb0fe7573d44f6bd39afadfca053e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f75bd02f6e704d2192a02b225e4826a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ab90f0b229574fcda8457ee7edc41ffe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c388c285f434486a9b006e60265ae915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15184d71dead470eb609f059ab393450":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d88320745e3144aeb3bb37ef615f32d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9460d1b1f84d4bb49fb4d5e9f0ee5d1b","IPY_MODEL_1db6cef6b5dd49b6bf31dbcd5abe4e87","IPY_MODEL_5078a2a45b554b6c8b20374b948fd514"],"layout":"IPY_MODEL_5774bbf3b3a34ac185c974f0bded1644"}},"9460d1b1f84d4bb49fb4d5e9f0ee5d1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36e1469c10294847a7b6e525c3b6160b","placeholder":"â€‹","style":"IPY_MODEL_13cbe7d2b6364adc81ce3c1f35183038","value":"vocabulary.txt:â€‡"}},"1db6cef6b5dd49b6bf31dbcd5abe4e87":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11ea737390184d6bafde270b2ae774e3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c308a69a798424794fdc6089233dd48","value":1}},"5078a2a45b554b6c8b20374b948fd514":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f89a3cccbc69436497fc0c3d4e977c19","placeholder":"â€‹","style":"IPY_MODEL_3ee42cb92b7b4c85aa9ca5a92f52727b","value":"â€‡460k/?â€‡[00:00&lt;00:00,â€‡13.9MB/s]"}},"5774bbf3b3a34ac185c974f0bded1644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36e1469c10294847a7b6e525c3b6160b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13cbe7d2b6364adc81ce3c1f35183038":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ea737390184d6bafde270b2ae774e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3c308a69a798424794fdc6089233dd48":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f89a3cccbc69436497fc0c3d4e977c19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ee42cb92b7b4c85aa9ca5a92f52727b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d518605723d44bf19b7a8c0e87ee25f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ffddf6122ce4d7886f15a2ee628b371","IPY_MODEL_2dab8b99c7e2484c835a1c3cdd509317","IPY_MODEL_6c00a587f52141cfbcf6c7f0b4e2b289"],"layout":"IPY_MODEL_d5d2643e7e814e428d0a168842cfdc97"}},"0ffddf6122ce4d7886f15a2ee628b371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b82ad0fc4bd43c19dab6fa65f52d43c","placeholder":"â€‹","style":"IPY_MODEL_d90476cf0de3475ca52c98e2bc8dd604","value":"config.json:â€‡"}},"2dab8b99c7e2484c835a1c3cdd509317":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b79aa47d0bc8474ab288af4758d2080c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c524f727588442a97e99f4de1b89cdb","value":1}},"6c00a587f52141cfbcf6c7f0b4e2b289":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_583f267b419c4c6b986fb47b55300bf4","placeholder":"â€‹","style":"IPY_MODEL_e217b7c8c5254f258c8e2259e8f68782","value":"â€‡2.37k/?â€‡[00:00&lt;00:00,â€‡200kB/s]"}},"d5d2643e7e814e428d0a168842cfdc97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b82ad0fc4bd43c19dab6fa65f52d43c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90476cf0de3475ca52c98e2bc8dd604":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b79aa47d0bc8474ab288af4758d2080c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6c524f727588442a97e99f4de1b89cdb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"583f267b419c4c6b986fb47b55300bf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e217b7c8c5254f258c8e2259e8f68782":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"832dbe7906194125ac982be3254548e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9d61acaa48b469ab7c6668ac4d0c435","IPY_MODEL_5b532466f07d4d55bd305ba717d2e09b","IPY_MODEL_b425279d57d54ac79089cc6ae41f90ac"],"layout":"IPY_MODEL_28e767265f284cd4bfc391664ea75aa6"}},"d9d61acaa48b469ab7c6668ac4d0c435":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c4ad304280d4698a841c1ad533d7cc4","placeholder":"â€‹","style":"IPY_MODEL_f940f82a28ef423ca7a4928848d60f69","value":"model.bin:â€‡100%"}},"5b532466f07d4d55bd305ba717d2e09b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_637666b8c0fb45249199f4a72e9d8050","max":483546902,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15d3547b70804bd69a8d71129e4a5c33","value":483546902}},"b425279d57d54ac79089cc6ae41f90ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29060677ad8e4c4db5c67df81f84cbd0","placeholder":"â€‹","style":"IPY_MODEL_c9f4b545053d4b1c8f1153051684351b","value":"â€‡484M/484Mâ€‡[00:01&lt;00:00,â€‡531MB/s]"}},"28e767265f284cd4bfc391664ea75aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c4ad304280d4698a841c1ad533d7cc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f940f82a28ef423ca7a4928848d60f69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"637666b8c0fb45249199f4a72e9d8050":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d3547b70804bd69a8d71129e4a5c33":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29060677ad8e4c4db5c67df81f84cbd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f4b545053d4b1c8f1153051684351b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}